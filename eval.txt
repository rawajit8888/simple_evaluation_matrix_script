import os
import pathlib
import re
import numpy as np
import pandas as pd
from tqdm import tqdm

import label_studio_sdk
import logging

from typing import List, Dict, Optional
from label_studio_ml.model import LabelStudioMLBase
from label_studio_ml.response import ModelResponse

import torch
import torch.optim as optim
import torch.nn as nn

from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
from torch.utils.data import RandomSampler
from torch.utils.data import SequentialSampler

from keras.utils import pad_sequences
from transformers import pipeline, Pipeline
from transformers import AdamW
from itertools import groupby
from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoTokenizer, AdamW
from transformers import DataCollatorForTokenClassification
from datasets import Dataset, ClassLabel, Value, Sequence, Features
from functools import partial
from dataprocessing.processlabels import ProcessLabels
from multitask_nn_model import MultiTaskNNModel
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix


class MultiTaskBertModel:
    LABEL_STUDIO_HOST = os.getenv('LABEL_STUDIO_HOST', 'http://localhost:8080')
    LABEL_STUDIO_API_KEY = os.getenv('LABEL_STUDIO_API_KEY', '5d1a32bb14129720a48ddeb09b5cc99f1e39cc7c')
    START_TRAINING_EACH_N_UPDATES = int(os.getenv('START_TRAINING_EACH_N_UPDATES', 10))
    LEARNING_RATE = float(os.getenv('LEARNING_RATE', 1e-3))
    NUM_TRAIN_EPOCHS = int(os.getenv('NUM_TRAIN_EPOCHS', 100))
    WEIGHT_DECAY = float(os.getenv('WEIGHT_DECAY', 0.01))

    def __init__(self, config, logger, label_interface=None, parsed_label_config=None):
        self.config = config
        self.logger = logger
        self.label_interface = label_interface
        self.parsed_label_config = parsed_label_config
        self.processed_label_encoders = ProcessLabels(
            self.parsed_label_config,
            pathlib.Path(self.config['MODEL_DIR'])
        )

    def reload_model(self):
        self.model = None
        classification_label_length = len(
            self.processed_label_encoders.labels["classification"]["encoder"].classes_
        ) if self.processed_label_encoders.labels["classification"]["encoder"] is not None else 1000
        sentiment_label_length = len(
            self.processed_label_encoders.labels["sentiment"]["encoder"].classes_
        ) if self.processed_label_encoders.labels["sentiment"]["encoder"] is not None else 3

        try:
            self.chk_path = str(pathlib.Path(self.config['MODEL_DIR']) / self.config['FINETUNED_MODEL_NAME'])
            self.finedtunnedmodelpath = f'.\\{self.chk_path}'

            self.logger.info(f"Loading finetuned model from {self.chk_path}")
            self.model = MultiTaskNNModel(self.finedtunnedmodelpath, classification_label_length, sentiment_label_length)
            self.model.LoadModel(self.finedtunnedmodelpath)
            self.tokenizer = AutoTokenizer.from_pretrained(self.finedtunnedmodelpath)

        except Exception as e:
            self.logger.info(f"Error Loading finetunned model: {e}")
            self.chk_path = str(pathlib.Path(self.config['MODEL_DIR']) / self.config['BASELINE_MODEL_NAME'])
            self.finedtunnedmodelpath = f'.\\{self.chk_path}'

            self.logger.info(f"Loading baseline model {self.chk_path}")
            self.model = MultiTaskNNModel(self.chk_path, classification_label_length, sentiment_label_length)
            self.tokenizer = AutoTokenizer.from_pretrained(self.chk_path)
            self.finedtunnedmodelpath = str(pathlib.Path(self.config['MODEL_DIR']) / self.config['FINETUNED_MODEL_NAME'])

    def fit(self, event, data, tasks, sample_mode=False, **kwargs):
        """Download dataset from Label Studio and prepare data for training in BERT"""
        if event not in ('ANNOTATION_CREATED', 'ANNOTATION_UPDATED', 'START_TRAINING'):
            self.logger.info(f"Skip training: event {event} is not supported")
            return

        ds_raw = []

        def getClassificationAttrName(attrs):
            return attrs == 'classification'

        def getSentimentAttrName(attrs):
            return attrs == 'sentiment'

        from_name_classification, to_name_classification, value_classification = \
            self.label_interface.get_first_tag_occurence('Taxonomy', 'HyperText', getClassificationAttrName)
        from_name_sentiment, to_name_sentiment, value_sentiment = \
            self.label_interface.get_first_tag_occurence('Taxonomy', 'HyperText', getSentimentAttrName)

        tokenizer = self.tokenizer

        for task in tasks:
            # SubQueryType comes from Label Studio data
            subquery_type = task['data'].get('SubQueryType', 'Unknown')
            for annotation in task['annotations']:
                if not annotation.get('result'):
                    continue

                # Extract labels from taxonomy
                sentiment_label = [r['value']['taxonomy'][0][0] for r in annotation['result']
                                   if r["from_name"] == "sentiment"][0]
                classification_label = " > ".join(
                    [r['value']['taxonomy'][0] for r in annotation['result'] if r["from_name"] == "classification"][0]
                )

                match = re.search(r"pre[^>]*>\s*(.*?)\s*</pre>", value_classification, re.DOTALL)
                value_classification_clean = match.group(1)[1:] if match else value_classification

                text = self.preload_task_data(task, task['data'][value_classification_clean])

                sentiment = self.processed_label_encoders['sentiment'].transform([sentiment_label])[0]
                classification = self.processed_label_encoders['classification'].transform([classification_label])[0]

                ds_raw.append([
                    text,
                    classification_label,
                    sentiment_label,
                    classification,
                    sentiment,
                    subquery_type
                ])

        self.logger.debug(f"Dataset: {ds_raw}")

        # Build dataframe
        df = pd.DataFrame(
            ds_raw,
            columns=["text", "classification_label", "sentiment_label",
                     "classification", "sentiment", "SubQueryType"]
        )

        # ---- CHANGED HERE: Use multi-level taxonomy for stratification ----
        stratify_col = df["classification_label"]  # Can also combine multiple columns if needed
        df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)
        if sample_mode:
            df = df.head(50)  # CHANGED HERE: small sample for test runs

        # Prepare stratified train/test indices
        train_indices = []
        test_indices = []
        rng = np.random.default_rng(42)

        for cat, group in df.groupby(stratify_col):
            idx = group.index.to_list()
            rng.shuffle(idx)
            n = len(idx)

            if n < 2:
                train_indices.extend(idx)
                continue

            n_train = int(np.floor(0.8 * n))
            if n_train == 0:
                n_train = 1

            train_indices.extend(idx[:n_train])
            test_indices.extend(idx[n_train:])

        # Fallback in case no test samples
        if len(test_indices) == 0 and len(df) > 1:
            all_idx = list(df.index)
            rng.shuffle(all_idx)
            split_idx = int(len(all_idx) * 0.8)
            train_indices = all_idx[:split_idx]
            test_indices = all_idx[split_idx:]

        train_df = df.loc[train_indices].reset_index(drop=True)
        test_df = df.loc[test_indices].reset_index(drop=True)

        self.logger.info(
            f"Total samples: {len(df)}, Train: {len(train_df)}, Test: {len(test_df)} (stratified by classification_label)"
        )

        # ---------- SAVE RAW TEST SPLIT ----------
        directory = os.path.dirname(self.finedtunnedmodelpath)
        eval_dir = os.path.join(directory, "evaluation")
        os.makedirs(eval_dir, exist_ok=True)

        raw_test_path = os.path.join(eval_dir, "test_raw_before_training.csv")
        raw_cols = ["text", "classification_label", "sentiment_label"]
        test_df[raw_cols].to_csv(raw_test_path, index=False)
        self.logger.info(f"Saved raw 20% test split (before training) to {raw_test_path}")

        MAX_LEN = 256
        batch_size = 16

        # ---------- TRAIN DATA ----------
        tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in train_df['text']]
        train_masks = [[int(token_id > 0) for token_id in input_id] for input_id in tokenized_texts]

        train_inputs = pad_sequences(tokenized_texts, maxlen=MAX_LEN, dtype='long', value=0,
                                     truncating='post', padding='post')
        train_masks = pad_sequences(train_masks, maxlen=MAX_LEN, dtype='long', value=0,
                                    truncating='post', padding='post')

        df_classification = train_df["classification"].astype(np.int64)
        df_sentiments = train_df['sentiment'].astype(np.int64)

        train_data = TensorDataset(
            torch.tensor(train_inputs),
            torch.tensor(train_masks),
            torch.tensor(df_classification.values),
            torch.tensor(df_sentiments.values)
        )
        train_sampler = RandomSampler(train_data)
        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

        model = self.model
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)

        optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
        criterion = nn.CrossEntropyLoss()

        print("fine-tuning in process")  # CHANGED HERE: log start of training

        with tqdm(total=self.NUM_TRAIN_EPOCHS, desc="Epochs") as pbar:
            try:
                for epoch in range(self.NUM_TRAIN_EPOCHS):
                    for step, batch in enumerate(train_dataloader):
                        model.train()
                        input_ids = batch[0].to(device)
                        attention_mask = batch[1].to(device)
                        classification_labels = batch[2].to(device)
                        sentiment_labels = batch[3].to(device)

                        optimizer.zero_grad()
                        classification_logits, sentiment_logits, _, _ = model(input_ids, attention_mask)

                        classification_loss = criterion(classification_logits, classification_labels)
                        sentiment_loss = criterion(sentiment_logits, sentiment_labels)

                        loss = classification_loss + sentiment_loss
                        loss.backward()
                        optimizer.step()

                        pbar.set_postfix(loss=loss.item())
                        pbar.update(1)
            except Exception as e:
                self.logger.error(str(e), exc_info=True)
                raise

        if not os.path.exists(directory):
            os.makedirs(directory)

        model.SaveModel(self.finedtunnedmodelpath)
        tokenizer.save_pretrained(self.finedtunnedmodelpath)

        # ---------- EVALUATION ----------
        if len(test_df) > 0:
            self.logger.info("Running evaluation on 20% test split...")

            model.eval()
            model.to(device)

            test_tokenized = [tokenizer.encode(text, add_special_tokens=True) for text in test_df["text"]]
            test_masks = [[int(token_id > 0) for token_id in input_id] for input_id in test_tokenized]

            test_inputs = pad_sequences(test_tokenized, maxlen=MAX_LEN, dtype='long', value=0,
                                        truncating='post', padding='post')
            test_masks = pad_sequences(test_masks, maxlen=MAX_LEN, dtype='long', value=0,
                                       truncating='post', padding='post')

            test_dataset = TensorDataset(
                torch.tensor(test_inputs),
                torch.tensor(test_masks),
            )
            test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)

            all_class_true_ids = test_df["classification"].astype(np.int64).tolist()
            all_sent_true_ids = test_df["sentiment"].astype(np.int64).tolist()

            all_class_pred_ids = []
            all_sent_pred_ids = []
            all_class_score = []
            all_sent_score = []

            with torch.no_grad():
                for batch in test_loader:
                    input_ids, attention_mask = [t.to(device) for t in batch]

                    classification_logits, sentiment_logits, _, _ = model(input_ids, attention_mask)

                    class_probs = nn.Softmax(dim=1)(classification_logits)
                    sent_probs = nn.Softmax(dim=1)(sentiment_logits)

                    class_preds = torch.argmax(class_probs, dim=1)
                    sent_preds = torch.argmax(sent_probs, dim=1)

                    all_class_pred_ids.extend(class_preds.cpu().numpy().tolist())
                    all_sent_pred_ids.extend(sent_preds.cpu().numpy().tolist())

                    all_class_score.extend(class_probs.gather(1, class_preds.unsqueeze(1)).squeeze(1).cpu().numpy().tolist())
                    all_sent_score.extend(sent_probs.gather(1, sent_preds.unsqueeze(1)).squeeze(1).cpu().numpy().tolist())

            # Decode labels
            class_encoder = self.processed_label_encoders['classification']
            sent_encoder = self.processed_label_encoders['sentiment']

            decoded_class_true = class_encoder.inverse_transform(np.array(all_class_true_ids))
            decoded_class_pred = class_encoder.inverse_transform(np.array(all_class_pred_ids))

            decoded_sent_true = sent_encoder.inverse_transform(np.array(all_sent_true_ids))
            decoded_sent_pred = sent_encoder.inverse_transform(np.array(all_sent_pred_ids))

            eval_df = pd.DataFrame({
                "text": test_df["text"],
                "SubQueryType": test_df["SubQueryType"],
                "classification_true": decoded_class_true,
                "classification_pred": decoded_class_pred,
                "classification_score": all_class_score,
                "sentiment_true": decoded_sent_true,
                "sentiment_pred": decoded_sent_pred,
                "sentiment_score": all_sent_score,
            })

            eval_path = os.path.join(eval_dir, "test_predictions.csv")
            eval_df.to_csv(eval_path, index=False)
            self.logger.info(f"Saved test predictions to {eval_path}")

            # Metrics
            class_accuracy = accuracy_score(decoded_class_true, decoded_class_pred)
            class_f1_weighted = f1_score(decoded_class_true, decoded_class_pred, average='weighted')
            class_report = classification_report(decoded_class_true, decoded_class_pred)

            sent_accuracy = accuracy_score(decoded_sent_true, decoded_sent_pred)
            sent_f1_weighted = f1_score(decoded_sent_true, decoded_sent_pred, average='weighted')
            sent_report = classification_report(decoded_sent_true, decoded_sent_pred)

            self.logger.info(f"Classification Accuracy: {class_accuracy:.4f}")
            self.logger.info(f"Classification F1 (weighted): {class_f1_weighted:.4f}")
            self.logger.info(f"Classification report:\n{class_report}")

            self.logger.info(f"Sentiment Accuracy: {sent_accuracy:.4f}")
            self.logger.info(f"Sentiment F1 (weighted): {sent_f1_weighted:.4f}")
            self.logger.info(f"Sentiment report:\n{sent_report}")

            metrics_path = os.path.join(eval_dir, "metrics.txt")
            with open(metrics_path, "w", encoding="utf-8") as f:
                f.write("=== Classification Metrics ===\n")
                f.write(f"Accuracy: {class_accuracy:.4f}\n")
                f.write(f"F1 (weighted): {class_f1_weighted:.4f}\n\n")
                f.write("Classification report:\n")
                f.write(class_report)
                f.write("\n\n=== Sentiment Metrics ===\n")
                f.write(f"Accuracy: {sent_accuracy:.4f}\n")
                f.write(f"F1 (weighted): {sent_f1_weighted:.4f}\n\n")
                f.write("Sentiment report:\n")
                f.write(sent_report)

            self.logger.info(f"Saved evaluation metrics to {metrics_path}")
