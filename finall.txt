# ============================================================
# FINAL EVALUATION SCRIPT FOR YOUR MULTITASK MODEL
# ============================================================

import torch
import torch.nn as nn
from transformers import BertModel, AutoTokenizer
import pandas as pd
import pickle
from sklearn.metrics import classification_report
import ast

# ============================================================
# 1. CONFIG
# ============================================================
CSV_FILE = "your_exported.csv"
TEXT_COL = "Subject"
LABEL_COL = "classification"
MODEL_DIR = "finetuned_multitask_model"         # the BERT folder
MODEL_PTH = "finetuned_multitask_model/multitask_model.pth"
ENCODER_FILE = "encoders/classification.labels.pkl"
BATCH_SIZE = 8


# ============================================================
# 2. LOAD LABEL ENCODER
# ============================================================
with open(ENCODER_FILE, "rb") as f:
    label_encoder = pickle.load(f)

num_labels = len(label_encoder.classes_)
print("Number of labels =", num_labels)


# ============================================================
# 3. YOUR MODEL ARCHITECTURE (RECREATED)
# ============================================================
class MultiTaskNNModel(nn.Module):
    def __init__(self, model_name, classification_label_length, sentiment_label_length=2):
        super(MultiTaskNNModel, self).__init__()

        self.bert = BertModel.from_pretrained(model_name, local_files_only=True)
        hidden = self.bert.config.hidden_size

        self.dropout = nn.Dropout(0.1)

        self.classifier = nn.Linear(hidden, classification_label_length)
        self.sentiment_classifier = nn.Linear(hidden, sentiment_label_length)

        self.classifier_softmax = nn.Softmax(dim=1)
        self.sentiment_softmax = nn.Softmax(dim=1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        pooled = outputs.pooler_output
        pooled = self.dropout(pooled)

        cls_logits = self.classifier(pooled)
        sent_logits = self.sentiment_classifier(pooled)

        cls_probs = self.classifier_softmax(cls_logits)
        sent_probs = self.sentiment_softmax(sent_logits)

        return cls_logits, cls_probs


# ============================================================
# 4. CREATE MODEL + LOAD WEIGHTS
# ============================================================
model = MultiTaskNNModel(
    model_name=MODEL_DIR,
    classification_label_length=num_labels
)

state = torch.load(MODEL_PTH, map_location="cpu")
model.load_state_dict(state, strict=False)
model.eval()


# ============================================================
# 5. CSV LABEL FIXER
# ============================================================
def extract_label(cell):
    if not isinstance(cell, str) or cell.strip() == "[]":
        return None
    try:
        parsed = ast.literal_eval(cell)
        taxonomy_list = parsed[0]["taxonomy"][0]
        return " > ".join(taxonomy_list)
    except:
        return None


df = pd.read_csv(CSV_FILE)
df["true_label"] = df[LABEL_COL].apply(extract_label)
df = df.dropna(subset=["true_label"])


# Convert true labels to ids
df["label_id"] = df["true_label"].apply(lambda x: label_encoder.transform([x])[0])


# ============================================================
# 6. TOKENIZER + ENCODING
# ============================================================
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)

texts = df[TEXT_COL].astype(str).tolist()
labels = df["label_id"].tolist()

enc = tokenizer(texts, padding=True, truncation=True, max_length=256, return_tensors="pt")


# ============================================================
# 7. RUN INFERENCE
# ============================================================
input_ids = enc["input_ids"]
attention_mask = enc["attention_mask"]

with torch.no_grad():
    cls_logits, cls_probs = model(input_ids, attention_mask)
    preds = torch.argmax(cls_probs, dim=1).numpy()

df["pred_label"] = label_encoder.inverse_transform(preds)


# ============================================================
# 8. METRICS
# ============================================================
print("\n====== CLASSIFICATION REPORT ======\n")
print(classification_report(df["true_label"], df["pred_label"], digits=4))

df.to_csv("evaluation_results.csv", index=False)
print("\nSaved: evaluation_results.csv")
