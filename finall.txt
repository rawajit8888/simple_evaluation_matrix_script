# ================================================
# Full Evaluation Script for Label Studio CSV
# Handles nested taxonomy automatically
# Generates predictions, metrics, and confusion matrix
# Safe: Offline, no backend interaction
# ================================================

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns
import json
import ast
import os

# --------------------------
# CONFIGURATION - UPDATE THIS
# --------------------------
MODEL_PATH = "C:/path_to_your_model"       # trained model folder
CSV_PATH = "C:/path_to_your_csv/emails.csv" # Label Studio exported CSV
TEXT_COL = "text"                           # column containing email/text
LABEL_COL = "classification"                # column containing taxonomy/label
SAVE_DIR = "C:/path_to_save_results"        # folder to save outputs

os.makedirs(SAVE_DIR, exist_ok=True)

# --------------------------
# FUNCTION TO EXTRACT READABLE LABEL
# --------------------------
def extract_label(cell_value):
    """
    Converts Label Studio classification column like:
    '[{"taxonomy":[["Internet Banking","Account Access","Unblock"]]}]'
    into readable label string:
    'Internet Banking > Account Access > Unblock'
    """
    if not cell_value or cell_value == "[]":
        return "Unknown"
    try:
        parsed = ast.literal_eval(cell_value)
        taxonomy_list = parsed[0]["taxonomy"][0]
        label = " > ".join(taxonomy_list)
        return label
    except Exception as e:
        return "Unknown"

# --------------------------
# 1. LOAD MODEL
# --------------------------
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
model.eval()
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
print("Model loaded on", device)

# --------------------------
# 2. LOAD CSV
# --------------------------
print("Loading CSV...")
df = pd.read_csv(CSV_PATH)
if TEXT_COL not in df.columns or LABEL_COL not in df.columns:
    raise ValueError(f"CSV must contain columns '{TEXT_COL}' and '{LABEL_COL}'")

texts = df[TEXT_COL].tolist()
y_true = [extract_label(x) for x in df[LABEL_COL]]
print(f"Loaded {len(texts)} rows")

# --------------------------
# 3. GENERATE PREDICTIONS
# --------------------------
print("Generating predictions...")
y_pred = []
y_conf = []

for text in texts:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        logits = model(**inputs).logits
    pred_id = torch.argmax(logits, dim=-1).item()
    prob = torch.softmax(logits, dim=-1)[0][pred_id].item()
    y_pred.append(pred_id)
    y_conf.append(prob)

# Convert prediction ids to labels if model has mapping
if hasattr(model.config, "id2label"):
    y_pred_labels = [model.config.id2label[p] for p in y_pred]
else:
    y_pred_labels = y_pred

# --------------------------
# 4. EVALUATION METRICS
# --------------------------
print("Computing metrics...")
report = classification_report(y_true, y_pred_labels, output_dict=True)
accuracy = accuracy_score(y_true, y_pred_labels)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred_labels, average='weighted')

print("\n=== CLASSIFICATION REPORT ===")
print(classification_report(y_true, y_pred_labels))
print(f"Accuracy: {accuracy:.4f}")
print(f"Weighted Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")

# --------------------------
# 5. CONFUSION MATRIX
# --------------------------
labels_sorted = sorted(list(set(y_true)))
cm = confusion_matrix(y_true, y_pred_labels, labels=labels_sorted)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels_sorted, yticklabels=labels_sorted)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
cm_path = os.path.join(SAVE_DIR, "confusion_matrix.png")
plt.savefig(cm_path)
plt.close()
print(f"Confusion matrix saved to {cm_path}")

# --------------------------
# 6. SAVE RESULTS
# --------------------------
# JSON summary
results = {
    "accuracy": float(accuracy),
    "weighted_precision": float(precision),
    "weighted_recall": float(recall),
    "weighted_f1": float(f1),
    "per_class": report,
    "y_true": y_true,
    "y_pred": y_pred_labels,
    "confidence_scores": y_conf
}
json_path = os.path.join(SAVE_DIR, "evaluation_results.json")
with open(json_path, "w") as f:
    json.dump(results, f, indent=4)
print(f"Evaluation results saved to {json_path}")

# CSV per-row
df_results = pd.DataFrame({
    "text": texts,
    "y_true": y_true,
    "y_pred": y_pred_labels,
    "confidence": y_conf
})
csv_path = os.path.join(SAVE_DIR, "evaluation_results.csv")
df_results.to_csv(csv_path, index=False)
print(f"Per-row results saved to {csv_path}")

print("\nâœ… Evaluation completed successfully!")






