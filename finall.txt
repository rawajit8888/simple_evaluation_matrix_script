import torch
import pandas as pd
import ast
import pickle
import torch.nn as nn
from transformers import BertTokenizer, BertModel

# ============================================================
# 1. CONFIG
# ============================================================
MODEL_DIR = "/content/fine_tuned_multitask"   # change if needed
ENCODER_PATH = "/content/fine_tuned_multitask/encoder/classification_sentiment.pkl"
CSV_PATH = "/content/data.csv"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# ============================================================
# 2. LOAD ENCODER
# ============================================================
print("\n===== LOADING ENCODER =====")

with open(ENCODER_PATH, "rb") as f:
    encoder = pickle.load(f)

print("Encoder loaded.")
print("Encoder type:", type(encoder))
print("Has classes_ ?", hasattr(encoder, "classes_"))
if hasattr(encoder, "classes_"):
    print("Number of classes:", len(encoder.classes_))
    print("Sample first 10:", encoder.classes_[:10])
else:
    print("ERROR: encoder has no classes_ → wrong file")


# ============================================================
# 3. MODEL CLASS (MATCH YOUR TRAINING CODE)
# ============================================================
class MultitaskNNModel(nn.Module):
    def __init__(self, model_name, classification_label_length=1000, sentiment_label_length=2):
        super(MultitaskNNModel, self).__init__()
        print("INIT CALLED → classification labels:", classification_label_length)

        self.bert = BertModel.from_pretrained(model_name, local_files_only=True)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, classification_label_length)

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = out.pooler_output
        pooled = self.dropout(pooled)
        logits = self.classifier(pooled)
        return logits


# ============================================================
# 4. LOAD MODEL
# ============================================================
print("\n===== LOADING MODEL =====")

tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)

# IMPORTANT: classification_label_length MUST match encoder
model = MultitaskNNModel(
    model_name=MODEL_DIR,
    classification_label_length=len(encoder.classes_)
)

model.load_state_dict(torch.load(f"{MODEL_DIR}/pytorch_model.bin", map_location=DEVICE))
model.to(DEVICE)
model.eval()

print("Model loaded.")
print("Model output classes (should match encoder):", model.classifier.out_features)


# ============================================================
# 5. LOAD CSV
# ============================================================
print("\n===== LOADING CSV =====")
df = pd.read_csv(CSV_PATH)
print("Rows loaded:", len(df))
print("Columns:", df.columns.tolist())


# ============================================================
# 6. EXTRACT LABEL FUNCTION
# ============================================================
def extract_label(cell_value):
    try:
        parsed = ast.literal_eval(cell_value)
        taxonomy = parsed[0]["taxonomy"][0]
        return " > ".join(taxonomy)
    except:
        return "Unknown"


# ============================================================
# 7. EVALUATION LOOP WITH FULL DEBUGGING
# ============================================================
correct = 0
total = 0

print("\n===== STARTING EVALUATION WITH DEBUG =====")

for idx, row in df.iterrows():

    text = str(row["html"])[:200]
    y_true = extract_label(row["classification"])

    # Tokenize
    inputs = tokenizer(text, return_tensors="pt", truncation=True,
                       padding=True, max_length=256).to(DEVICE)

    # Predict
    with torch.no_grad():
        logits = model(**inputs)
        pred_id = torch.argmax(logits, dim=1).item()

    # ===========================
    # DEBUG BLOCK
    # ===========================
    print("\n---------------------------------------------------")
    print(f"Row {idx}")
    print("Text preview:", text)
    print("y_true:", y_true)
    print("Pred_id:", pred_id)
    print("Logits shape:", logits.shape)
    print("Logits:", logits.cpu().numpy())

    print("\n--- ENCODER DEBUG ---")
    print("Encoder type:", type(encoder))
    print("Has classes_:", hasattr(encoder, "classes_"))
    if hasattr(encoder, "classes_"):
        print("Encoder classes length:", len(encoder.classes_))

    # ===========================
    # Decode pred_id → label
    # ===========================
    try:
        pred_label = encoder.inverse_transform([pred_id])[0]
        print("Mapped Pred Label:", pred_label)
    except Exception as e:
        print("❌ ERROR DURING DECODING")
        print("Predicted ID:", pred_id)
        print("Exception:", e)
        print("Classes count:", len(encoder.classes_))
        print("Valid ID range: 0 →", len(encoder.classes_) - 1)
        raise

    total += 1
    if pred_label == y_true:
        correct += 1

    if idx == 20:
        break

# ============================================================
# 8. FINAL ACCURACY
# ============================================================
print("\n===== FINAL ACCURACY =====")
print(f"{correct}/{total}  =  {correct/total:.4f}")
