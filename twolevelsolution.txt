def fit(self, event, data, tasks, **kwargs):
        """
        3-Level Hierarchical Training Pipeline
        
        Level 1: Email ‚Üí MasterDepartment
        Level 2: Email + MasterDepartment ‚Üí Department
        Level 3: Email + MasterDepartment + Department ‚Üí QueryType
        """
        
        self.logger.info("=" * 80)
        self.logger.info("üöÄ 3-LEVEL HIERARCHICAL TRAINING INITIATED")
        self.logger.info("=" * 80)
        
        # Initialize database
        self.init_metrics_db()
        
        # ========== PREPARE DATA ==========
        self.logger.info("üìä Preparing training data...")
        
        train_data = []
        for task in tasks:
            extracted = self._extract_taxonomy_from_task(task)
            if extracted and extracted['text']:
                train_data.append(extracted)
        
        if len(train_data) == 0:
            self.logger.error("‚ùå No valid training data found")
            self.logger.error("Please check your Label Studio annotations:")
            self.logger.error("  - Ensure tasks have 'masterdepartment' labels")
            self.logger.error("  - Check that taxonomy fields are properly configured")
            return
        
        self.logger.info(f"‚úì Prepared {len(train_data)} training samples")
        
        # Log sample for debugging
        if train_data:
            sample = train_data[0]
            self.logger.info(f"üìù Sample annotation:")
            self.logger.info(f"   MasterDepartment: {sample['masterdepartment']}")
            self.logger.info(f"   Department: {sample['department']}")
            self.logger.info(f"   QueryType: {sample['querytype']}")
        
        # Log distribution
        masterdept_count = sum(1 for d in train_data if d['masterdepartment'])
        dept_count = sum(1 for d in train_data if d['department'])
        qt_count = sum(1 for d in train_data if d['querytype'])
        
        self.logger.info(f"üìä Label distribution:")
        self.logger.info(f"   Level 1 (MasterDepartment): {masterdept_count} samples")
        self.logger.info(f"   Level 2 (Department): {dept_count} samples")
        self.logger.info(f"   Level 3 (QueryType): {qt_count} samples")
        
        # Convert to DataFrame
        df = pd.DataFrame(train_data)
        
        from sklearn.model_selection import train_test_split
        
        # ========== TRAIN LEVEL 1: MasterDepartment ONLY ==========
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("üéØ LEVEL 1 TRAINING: MasterDepartment ONLY")
        self.logger.info("=" * 80)
        
        # Split for Level 1
        class_counts = df['masterdepartment'].value_counts()
        min_samples = class_counts.min()
        
        if min_samples >= 2:
            self.logger.info(f"‚úì Using stratified split (min class size: {min_samples})")
            try:
                level1_train_df, level1_test_df = train_test_split(
                    df, test_size=0.2, random_state=42, 
                    stratify=df['masterdepartment']
                )
            except ValueError as e:
                self.logger.warning(f"‚ö†Ô∏è  Stratified split failed: {e}")
                self.logger.warning("‚ö†Ô∏è  Falling back to random split")
                level1_train_df, level1_test_df = train_test_split(
                    df, test_size=0.2, random_state=42
                )
        else:
            self.logger.warning(f"‚ö†Ô∏è  Some classes have only 1 sample - using random split")
            level1_train_df, level1_test_df = train_test_split(
                df, test_size=0.2, random_state=42
            )
        
        self.logger.info(f"üìä Level 1 - Train: {len(level1_train_df)} | Test: {len(level1_test_df)}")
        self._train_level1(level1_train_df, level1_test_df)
        
        # ========== TRAIN LEVEL 2: Department ==========
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("üéØ LEVEL 2 TRAINING: Department")
        self.logger.info("=" * 80)
        
        # Filter samples with department labels FIRST
        dept_df = df[df['department'].notna()].copy()
        
        if len(dept_df) > 0:
            self.logger.info(f"üìä Department samples: {len(dept_df)}")
            
            # NOW split the filtered data
            dept_class_counts = dept_df['department'].value_counts()
            dept_min_samples = dept_class_counts.min()
            
            if dept_min_samples >= 2:
                self.logger.info(f"‚úì Using stratified split for departments (min class size: {dept_min_samples})")
                try:
                    dept_train_df, dept_test_df = train_test_split(
                        dept_df, test_size=0.2, random_state=42,
                        stratify=dept_df['department']
                    )
                except ValueError as e:
                    self.logger.warning(f"‚ö†Ô∏è  Stratified split failed: {e}")
                    self.logger.warning("‚ö†Ô∏è  Falling back to random split")
                    dept_train_df, dept_test_df = train_test_split(
                        dept_df, test_size=0.2, random_state=42
                    )
            else:
                self.logger.warning(f"‚ö†Ô∏è  Some department classes have only 1 sample - using random split")
                dept_train_df, dept_test_df = train_test_split(
                    dept_df, test_size=0.2, random_state=42
                )
            
            self.logger.info(f"üìä Level 2 - Train: {len(dept_train_df)} | Test: {len(dept_test_df)}")
            self._train_level2(dept_train_df, dept_test_df)
        else:
            self.logger.warning("‚ö†Ô∏è  No Department labels found - skipping Level 2 training")
        
        # ========== TRAIN LEVEL 3: QueryType ==========
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("üéØ LEVEL 3 TRAINING: QueryType")
        self.logger.info("=" * 80)
        
        # Filter samples with querytype labels FIRST
        qt_df = df[df['querytype'].notna()].copy()
        
        if len(qt_df) > 0:
            self.logger.info(f"üìä QueryType samples: {len(qt_df)}")
            
            # NOW split the filtered data
            qt_class_counts = qt_df['querytype'].value_counts()
            qt_min_samples = qt_class_counts.min()
            
            if qt_min_samples >= 2:
                self.logger.info(f"‚úì Using stratified split for querytypes (min class size: {qt_min_samples})")
                try:
                    qt_train_df, qt_test_df = train_test_split(
                        qt_df, test_size=0.2, random_state=42,
                        stratify=qt_df['querytype']
                    )
                except ValueError as e:
                    self.logger.warning(f"‚ö†Ô∏è  Stratified split failed: {e}")
                    self.logger.warning("‚ö†Ô∏è  Falling back to random split")
                    qt_train_df, qt_test_df = train_test_split(
                        qt_df, test_size=0.2, random_state=42
                    )
            else:
                self.logger.warning(f"‚ö†Ô∏è  Some querytype classes have only 1 sample - using random split")
                qt_train_df, qt_test_df = train_test_split(
                    qt_df, test_size=0.2, random_state=42
                )
            
            self.logger.info(f"üìä Level 3 - Train: {len(qt_train_df)} | Test: {len(qt_test_df)}")
            self._train_level3(qt_train_df, qt_test_df)
        else:
            self.logger.warning("‚ö†Ô∏è  No QueryType labels found - skipping Level 3 training")
        
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("üéâ 3-LEVEL HIERARCHICAL TRAINING COMPLETE")
        self.logger.info("=" * 80)







#evaluatin matrix save to folder 

def save_evaluation_to_files(self, level, level_name, test_df, true_labels, pred_labels, accuracy, f1):
        """
        Save evaluation results to files:
        - CSV with test predictions
        - Text file with classification report
        - Text file with confusion matrix
        """
        from sklearn.metrics import classification_report, confusion_matrix
        
        # Create evaluation folder
        eval_dir = os.path.join(self.model_dir, "evaluation", level_name)
        os.makedirs(eval_dir, exist_ok=True)
        
        self.logger.info(f"üìÅ Creating evaluation folder: {eval_dir}")
        
        # ========== SAVE TEST PREDICTIONS TO CSV ==========
        predictions_df = test_df.copy()
        predictions_df['true_label'] = true_labels
        predictions_df['predicted_label'] = pred_labels
        predictions_df['correct'] = predictions_df['true_label'] == predictions_df['predicted_label']
        
        csv_path = os.path.join(eval_dir, f"{level_name}_test_predictions.csv")
        predictions_df.to_csv(csv_path, index=False)
        self.logger.info(f"üíæ Test predictions saved to: {csv_path}")
        
        # ========== SAVE CLASSIFICATION REPORT ==========
        report = classification_report(true_labels, pred_labels, zero_division=0)
        report_path = os.path.join(eval_dir, f"{level_name}_classification_report.txt")
        
        with open(report_path, 'w') as f:
            f.write(f"=" * 80 + "\n")
            f.write(f"CLASSIFICATION REPORT - {level_name.upper()}\n")
            f.write(f"=" * 80 + "\n")
            f.write(f"Overall Accuracy: {accuracy:.4f}\n")
            f.write(f"Overall F1-Score: {f1:.4f}\n")
            f.write(f"=" * 80 + "\n\n")
            f.write(report)
        
        self.logger.info(f"üíæ Classification report saved to: {report_path}")
        
        # ========== SAVE CONFUSION MATRIX ==========
        cm = confusion_matrix(true_labels, pred_labels)
        cm_path = os.path.join(eval_dir, f"{level_name}_confusion_matrix.txt")
        
        with open(cm_path, 'w') as f:
            f.write(f"=" * 80 + "\n")
            f.write(f"CONFUSION MATRIX - {level_name.upper()}\n")
            f.write(f"=" * 80 + "\n\n")
            f.write(str(cm))
            f.write("\n")
        
        self.logger.info(f"üíæ Confusion matrix saved to: {cm_path}")
        
        # ========== SAVE SUMMARY ==========
        summary_path = os.path.join(eval_dir, f"{level_name}_summary.txt")
        
        with open(summary_path, 'w') as f:
            f.write(f"=" * 80 + "\n")
            f.write(f"EVALUATION SUMMARY - {level_name.upper()}\n")
            f.write(f"=" * 80 + "\n")
            f.write(f"Level: {level}\n")
            f.write(f"Training samples: {len(test_df)}\n")
            f.write(f"Accuracy: {accuracy:.4f}\n")
            f.write(f"F1-Score: {f1:.4f}\n")
            f.write(f"Number of classes: {len(set(true_labels))}\n")
            f.write(f"Correct predictions: {sum(predictions_df['correct'])}\n")
            f.write(f"Incorrect predictions: {sum(~predictions_df['correct'])}\n")
            f.write(f"=" * 80 + "\n")
        
        self.logger.info(f"üíæ Summary saved to: {summary_path}")
        self.logger.info(f"‚úÖ Evaluation files created in: {eval_dir}")





# Save evaluation to files
        self.save_evaluation_to_files(
            level=1,
            level_name="masterdepartment",
            test_df=test_df,
            true_labels=decoded_masterdept_true,
            pred_labels=decoded_masterdept_pred,
            accuracy=masterdept_accuracy,
            f1=masterdept_f1
        )










# Save evaluation to files
        self.save_evaluation_to_files(
            level=2,
            level_name="department",
            test_df=test_df,
            true_labels=decoded_dept_true,
            pred_labels=decoded_dept_pred,
            accuracy=dept_accuracy,
            f1=dept_f1
        )











# Save evaluation to files
        self.save_evaluation_to_files(
            level=3,
            level_name="querytype",
            test_df=test_df,
            true_labels=decoded_qt_true,
            pred_labels=decoded_qt_pred,
            accuracy=qt_accuracy,
            f1=qt_f1
        )
```

---

## **Summary of Changes:**

**1. Line ~768**: Add the new `save_evaluation_to_files()` method (65 lines)

**2. Line 1352**: Add evaluation file saving call for Level 1

**3. Line 1489**: Add evaluation file saving call for Level 2

**4. Line 1626**: Add evaluation file saving call for Level 3

---

## **What This Creates:**

After training, you'll get this folder structure:
```
./results/bert-classification-sentiment3level_training/
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ masterdepartment/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ masterdepartment_test_predictions.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ masterdepartment_classification_report.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ masterdepartment_confusion_matrix.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ masterdepartment_summary.txt
‚îÇ   ‚îú‚îÄ‚îÄ department/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ department_test_predictions.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ department_classification_report.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ department_confusion_matrix.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ department_summary.txt
‚îÇ   ‚îî‚îÄ‚îÄ querytype/
‚îÇ       ‚îú‚îÄ‚îÄ querytype_test_predictions.csv
‚îÇ       ‚îú‚îÄ‚îÄ querytype_classification_report.txt
‚îÇ       ‚îú‚îÄ‚îÄ querytype_confusion_matrix.txt
‚îÇ       ‚îî‚îÄ‚îÄ querytype_summary.txt
‚îú‚îÄ‚îÄ metrics.db  (SQLite database - still created)
‚îú‚îÄ‚îÄ masterdepartment_model/
‚îú‚îÄ‚îÄ department_model/
‚îî‚îÄ‚îÄ querytype_model/














#prediction server error 500
def predict(self, tasks: List[Dict], texts: str, context: Optional[Dict] = None, **kwargs):
        """
        3-LEVEL HIERARCHICAL INFERENCE PIPELINE
        
        Just pass email text, automatically routes through:
        1. Level 1: Email ‚Üí MasterDepartment
        2. Level 2: Email + MasterDepartment ‚Üí Department
        3. Level 3: Email + MasterDepartment + Department ‚Üí QueryType
        
        Returns all predictions in a single result.
        """
        
        try:
            self.logger.info("=" * 80)
            self.logger.info("üîç PREDICTION REQUEST RECEIVED")
            self.logger.info("=" * 80)
            
            # ========== VALIDATION ==========
            if self.model is None:
                error_msg = "‚ùå Level 1 model not loaded - cannot make predictions"
                self.logger.error(error_msg)
                return []
            
            if self.tokenizer is None:
                error_msg = "‚ùå Tokenizer not loaded - cannot make predictions"
                self.logger.error(error_msg)
                return []
            
            # NOTE: label_interface is optional - only needed for Label Studio mode
            
            self.logger.info("‚úì All required components loaded")

            # ========== GET LABEL STUDIO TAGS (Optional for external calls) ==========
            try:
                if self.label_interface is not None:
                    # Label Studio mode - get proper tags
                    def getMasterDepartmentAttrName(attrs):
                        return attrs == "masterdepartment"

                    def getDepartmentAttrName(attrs):
                        return attrs == "department"

                    def getQueryTypeAttrName(attrs):
                        return attrs == "querytype"

                    from_name_masterdepartment, to_name_masterdepartment, _ = \
                        self.label_interface.get_first_tag_occurence(
                            'Taxonomy', 'HyperText', getMasterDepartmentAttrName
                        )

                    from_name_department, to_name_department, _ = \
                        self.label_interface.get_first_tag_occurence(
                            "Taxonomy", "HyperText", getDepartmentAttrName
                        )

                    from_name_querytype, to_name_querytype, _ = \
                        self.label_interface.get_first_tag_occurence(
                            "Taxonomy", "HyperText", getQueryTypeAttrName
                        )
                    
                    self.logger.info("‚úì Label Studio tags retrieved successfully")
                else:
                    # External API mode - use default tags
                    from_name_masterdepartment = "masterdepartment"
                    to_name_masterdepartment = "text"
                    from_name_department = "department"
                    to_name_department = "text"
                    from_name_querytype = "querytype"
                    to_name_querytype = "text"
                    
                    self.logger.info("‚úì Using default tags (external API mode)")
                
            except Exception as e:
                self.logger.error(f"‚ùå Error getting tags: {e}")
                # Use defaults as fallback
                from_name_masterdepartment = "masterdepartment"
                to_name_masterdepartment = "text"
                from_name_department = "department"
                to_name_department = "text"
                from_name_querytype = "querytype"
                to_name_querytype = "text"
                self.logger.info("‚úì Using fallback default tags")

            # ========== EXTRACT TEXT FROM INPUT ==========
            try:
                self.logger.info(f"üìù Input type: {type(texts)}")
                self.logger.info(f"üìù Input length: {len(texts) if isinstance(texts, (list, str)) else 'N/A'}")
                
                text_list = []
                
                if isinstance(texts, str):
                    # Single string
                    text_list = [texts]
                    self.logger.info("‚úì Processing single string input")
                    
                elif isinstance(texts, list):
                    # List of texts or dicts
                    for idx, text in enumerate(texts):
                        if isinstance(text, dict):
                            if 'text' in text:
                                text_list.append(text['text'])
                            elif 'html' in text:
                                text_list.append(text['html'])
                            else:
                                text_list.append(str(text))
                        elif isinstance(text, str):
                            text_list.append(text)
                        else:
                            text_list.append(str(text))
                    
                    self.logger.info(f"‚úì Processing list with {len(text_list)} items")
                    
                elif isinstance(texts, dict):
                    # Single dict
                    if 'text' in texts:
                        text_list = [texts['text']]
                    elif 'html' in texts:
                        text_list = [texts['html']]
                    else:
                        text_list = [str(texts)]
                    
                    self.logger.info("‚úì Processing single dict input")
                    
                else:
                    # Fallback
                    text_list = [str(texts)]
                    self.logger.warning(f"‚ö†Ô∏è  Unknown input type, converting to string")
                
                if not text_list or all(not t for t in text_list):
                    error_msg = "‚ùå No valid text found in input"
                    self.logger.error(error_msg)
                    return []
                
                self.logger.info(f"üìä Successfully extracted {len(text_list)} text(s) for processing")
                
                # Log first text sample for debugging
                if text_list:
                    sample_text = text_list[0][:100] + "..." if len(text_list[0]) > 100 else text_list[0]
                    self.logger.info(f"üìÑ Sample text: {sample_text}")
                
            except Exception as e:
                self.logger.error(f"‚ùå Error extracting text from input: {e}")
                self.logger.exception("Full traceback:")
                return []

            # ========== TOKENIZATION ==========
            try:
                tokenizer = self.tokenizer
                MAX_LEN = 256
                batch_size = 16

                self.logger.info(f"üî§ Tokenizing {len(text_list)} text(s)...")
                
                # Tokenize and pad
                _inputs, _masks = self._tokenize_and_pad(text_list, tokenizer, MAX_LEN)
                
                self.logger.info(f"‚úì Tokenization complete - Shape: {_inputs.shape}")

                # Create DataLoader
                dataloader = self._create_dataloader(
                    _inputs, _masks, batch_size=batch_size, shuffle=False
                )
                
                self.logger.info(f"‚úì DataLoader created - {len(dataloader)} batches")
                
            except Exception as e:
                self.logger.error(f"‚ùå Error during tokenization: {e}")
                self.logger.exception("Full traceback:")
                return []

            # ========== SETUP MODELS ==========
            try:
                device = self._get_device()
                self.logger.info(f"üñ•Ô∏è  Using device: {device}")
                
                # Move models to device and set to eval mode
                self.model.to(device)
                self.model.eval()
                self.logger.info("‚úì Level 1 model ready")

                if self.department_model is not None:
                    self.department_model.to(device)
                    self.department_model.eval()
                    self.logger.info("‚úì Level 2 model ready - Department predictions enabled")
                else:
                    self.logger.info("‚ö†Ô∏è  Level 2 model not loaded - Department predictions will be skipped")

                if self.querytype_model is not None:
                    self.querytype_model.to(device)
                    self.querytype_model.eval()
                    self.logger.info("‚úì Level 3 model ready - QueryType predictions enabled")
                else:
                    self.logger.info("‚ö†Ô∏è  Level 3 model not loaded - QueryType predictions will be skipped")
                    
            except Exception as e:
                self.logger.error(f"‚ùå Error setting up models: {e}")
                self.logger.exception("Full traceback:")
                return []

            # ========== RUN INFERENCE ==========
            try:
                self.logger.info("üöÄ Starting inference...")
                
                predictions = []
                global_text_idx = 0

                with torch.no_grad():
                    for batch_idx, batch in enumerate(dataloader):
                        self.logger.info(f"   Processing batch {batch_idx + 1}/{len(dataloader)}...")
                        
                        input_ids, attention_mask = [t.to(device) for t in batch]
            
                        # ========== LEVEL 1: MasterDepartment ==========
                        try:
                            masterdept_logits = self.model(input_ids, attention_mask)
                            masterdept_probs = torch.softmax(masterdept_logits, dim=1)
                            masterdept_preds = torch.argmax(masterdept_probs, dim=1)

                            decoded_masterdept_preds = self._decode_predictions(
                                masterdept_preds, 'masterdepartment'
                            )
                            
                            self.logger.info(f"   ‚úì Level 1 predictions: {decoded_masterdept_preds}")
                            
                        except Exception as e:
                            self.logger.error(f"‚ùå Error in Level 1 prediction: {e}")
                            self.logger.exception("Full traceback:")
                            raise

                        for i in range(len(masterdept_preds)):
                            try:
                                current_text = text_list[global_text_idx]
                                global_text_idx += 1

                                # ---------- MASTER DEPARTMENT ----------
                                predictions.append({
                                    "from_name": from_name_masterdepartment,
                                    "to_name": to_name_masterdepartment,
                                    "type": "taxonomy",
                                    "value": {
                                        "taxonomy": [
                                            [decoded_masterdept_preds[i]]
                                        ],
                                        "score": float(masterdept_probs[i][masterdept_preds[i]].item())
                                    }
                                })

                                # ========== LEVEL 2: DEPARTMENT ==========
                                if self.department_model is not None:
                                    try:
                                        conditional_text = (
                                            f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                            f"Email: {current_text}"
                                        )

                                        dept_ids = tokenizer.encode(
                                            conditional_text,
                                            add_special_tokens=True,
                                            max_length=256,
                                            truncation=True
                                        )
                                        dept_mask = [int(t > 0) for t in dept_ids]

                                        dept_logits = self.department_model(
                                            torch.tensor([dept_ids]).to(device),
                                            torch.tensor([dept_mask]).to(device)
                                        )

                                        if isinstance(dept_logits, (tuple, list)):
                                            dept_logits = dept_logits[0]

                                        dept_probs = torch.softmax(dept_logits, dim=1)
                                        dept_idx = torch.argmax(dept_probs, dim=1).item()

                                        department_label = self._decode_predictions(
                                            [dept_idx], "department"
                                        )[0]

                                        predictions.append({
                                            "from_name": from_name_department,
                                            "to_name": to_name_department,
                                            "type": "taxonomy",
                                            "value": {
                                                "taxonomy": [department_label.split(" > ")],
                                                "score": float(dept_probs[0][dept_idx].item())
                                            }
                                        })
                                        
                                        self.logger.info(f"   ‚úì Level 2 prediction: {department_label}")

                                        # ========== LEVEL 3: QUERY TYPE ==========
                                        if self.querytype_model is not None:
                                            try:
                                                conditional_text_qt = (
                                                    f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                                    f"Department: {department_label} "
                                                    f"Email: {current_text}"
                                                )

                                                qt_ids = tokenizer.encode(
                                                    conditional_text_qt,
                                                    add_special_tokens=True,
                                                    max_length=256,
                                                    truncation=True
                                                )
                                                qt_mask = [int(t > 0) for t in qt_ids]

                                                qt_logits = self.querytype_model(
                                                    torch.tensor([qt_ids]).to(device),
                                                    torch.tensor([qt_mask]).to(device)
                                                )

                                                if isinstance(qt_logits, (tuple, list)):
                                                    qt_logits = qt_logits[0]

                                                qt_probs = torch.softmax(qt_logits, dim=1)
                                                qt_idx = torch.argmax(qt_probs, dim=1).item()

                                                querytype_label = self._decode_predictions(
                                                    [qt_idx], "querytype"
                                                )[0]

                                                predictions.append({
                                                    "from_name": from_name_querytype,
                                                    "to_name": to_name_querytype,
                                                    "type": "taxonomy",
                                                    "value": {
                                                        "taxonomy": [querytype_label.split(" > ")],
                                                        "score": float(qt_probs[0][qt_idx].item())
                                                    }
                                                })
                                                
                                                self.logger.info(f"   ‚úì Level 3 prediction: {querytype_label}")
                                                
                                            except Exception as e:
                                                self.logger.error(f"‚ùå Error in Level 3 prediction for item {i}: {e}")
                                                self.logger.exception("Full traceback:")
                                                # Continue without Level 3 prediction
                                                
                                    except Exception as e:
                                        self.logger.error(f"‚ùå Error in Level 2 prediction for item {i}: {e}")
                                        self.logger.exception("Full traceback:")
                                        # Continue without Level 2 and Level 3 predictions
                                        
                            except Exception as e:
                                self.logger.error(f"‚ùå Error processing item {i} in batch: {e}")
                                self.logger.exception("Full traceback:")
                                # Continue to next item

                self.logger.info("=" * 80)
                self.logger.info(f"‚úÖ INFERENCE COMPLETE - Generated {len(predictions)} predictions")
                self.logger.info("=" * 80)
                
                return predictions
                
            except Exception as e:
                self.logger.error(f"‚ùå Error during inference loop: {e}")
                self.logger.exception("Full traceback:")
                return []
            
        except Exception as e:
            # Catch-all for any unexpected errors
            self.logger.error("=" * 80)
            self.logger.error("‚ùå CRITICAL ERROR IN PREDICT METHOD")
            self.logger.error("=" * 80)
            self.logger.error(f"Error: {e}")
            self.logger.exception("Full traceback:")
            
            # Return empty list instead of crashing
            return []


# change logic for level 3 training

# ========== LEVEL 3: QUERY TYPE ==========
                                        if self.querytype_model is not None:
                                            try:
                                                conditional_text_qt = (
                                                    f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                                    f"Department: {department_label} "
                                                    f"Email: {current_text}"
                                                )

                                                qt_ids = tokenizer.encode(
                                                    conditional_text_qt,
                                                    add_special_tokens=True,
                                                    max_length=256,
                                                    truncation=True
                                                )
                                                qt_mask = [int(t > 0) for t in qt_ids]

                                                qt_logits = self.querytype_model(
                                                    torch.tensor([qt_ids]).to(device),
                                                    torch.tensor([qt_mask]).to(device)
                                                )

                                                if isinstance(qt_logits, (tuple, list)):
                                                    qt_logits = qt_logits[0]

                                                qt_probs = torch.softmax(qt_logits, dim=1)
                                                
                                                # ========== HIERARCHICAL CONSTRAINT ==========
                                                qt_encoder = self.querytype_model.get_encoder()
                                                all_querytypes = qt_encoder.classes_
                                                
                                                # Filter to only QueryTypes that start with current department
                                                valid_indices = []
                                                valid_labels = []
                                                
                                                for idx, qt_label in enumerate(all_querytypes):
                                                    if qt_label.startswith(department_label):
                                                        valid_indices.append(idx)
                                                        valid_labels.append(qt_label)
                                                
                                                if len(valid_indices) == 0:
                                                    self.logger.warning(f"‚ö†Ô∏è  No QueryTypes found for department: {department_label}")
                                                    continue
                                                
                                                # Get probabilities only for valid indices
                                                valid_probs = qt_probs[0][valid_indices]
                                                
                                                # Find the highest probability among valid options
                                                best_valid_idx = torch.argmax(valid_probs).item()
                                                qt_idx = valid_indices[best_valid_idx]
                                                querytype_label = valid_labels[best_valid_idx]
                                                final_score = valid_probs[best_valid_idx].item()
                                                
                                                # Log the constraint application
                                                original_prediction_idx = torch.argmax(qt_probs, dim=1).item()
                                                original_prediction = all_querytypes[original_prediction_idx]
                                                
                                                if original_prediction_idx != qt_idx:
                                                    self.logger.warning(f"‚ö†Ô∏è  Hierarchical constraint applied!")
                                                    self.logger.warning(f"   Original: {original_prediction} ({qt_probs[0][original_prediction_idx].item():.3f})")
                                                    self.logger.warning(f"   Corrected: {querytype_label} ({final_score:.3f})")
                                                else:
                                                    self.logger.info(f"   ‚úì Level 3: {querytype_label} ({final_score:.3f})")

                                                predictions.append({
                                                    "from_name": from_name_querytype,
                                                    "to_name": to_name_querytype,
                                                    "type": "taxonomy",
                                                    "value": {
                                                        "taxonomy": [querytype_label.split(" > ")],
                                                        "score": float(final_score)
                                                    }
                                                })
                                                
                                            except Exception as e:
                                                self.logger.error(f"‚ùå Error in Level 3 prediction for item {i}: {e}")
                                                self.logger.exception("Full traceback:")





#changed logic for level 2 predictions
# ========== LEVEL 2: DEPARTMENT ==========
                                    if self.department_model is not None:
                                        try:
                                            conditional_text = (
                                                f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                                f"Email: {current_text}"
                                            )

                                            dept_ids = tokenizer.encode(
                                                conditional_text,
                                                add_special_tokens=True,
                                                max_length=256,
                                                truncation=True
                                            )
                                            dept_mask = [int(t > 0) for t in dept_ids]

                                            dept_logits = self.department_model(
                                                torch.tensor([dept_ids]).to(device),
                                                torch.tensor([dept_mask]).to(device)
                                            )

                                            if isinstance(dept_logits, (tuple, list)):
                                                dept_logits = dept_logits[0]

                                            dept_probs = torch.softmax(dept_logits, dim=1)
                                            
                                            # ========== HIERARCHICAL CONSTRAINT FOR LEVEL 2 ==========
                                            dept_encoder = self.department_model.get_encoder()
                                            all_departments = dept_encoder.classes_
                                            
                                            current_masterdept = decoded_masterdept_preds[i]
                                            
                                            # Filter to only Departments that start with current MasterDepartment
                                            valid_indices = []
                                            valid_labels = []
                                            
                                            for idx, dept_label in enumerate(all_departments):
                                                if dept_label.startswith(current_masterdept):
                                                    valid_indices.append(idx)
                                                    valid_labels.append(dept_label)
                                            
                                            if len(valid_indices) == 0:
                                                self.logger.warning(f"‚ö†Ô∏è  No Departments found for MasterDepartment: {current_masterdept}")
                                                continue
                                            
                                            # Get probabilities only for valid indices
                                            valid_probs = dept_probs[0][valid_indices]
                                            
                                            # Find the highest probability among valid options
                                            best_valid_idx = torch.argmax(valid_probs).item()
                                            dept_idx = valid_indices[best_valid_idx]
                                            department_label = valid_labels[best_valid_idx]
                                            final_score = valid_probs[best_valid_idx].item()
                                            
                                            # Log the constraint application
                                            original_prediction_idx = torch.argmax(dept_probs, dim=1).item()
                                            original_prediction = all_departments[original_prediction_idx]
                                            
                                            if original_prediction_idx != dept_idx:
                                                self.logger.warning(f"‚ö†Ô∏è  Hierarchical constraint applied at Level 2!")
                                                self.logger.warning(f"   MasterDepartment: {current_masterdept}")
                                                self.logger.warning(f"   Original: {original_prediction} ({dept_probs[0][original_prediction_idx].item():.3f})")
                                                self.logger.warning(f"   Corrected: {department_label} ({final_score:.3f})")
                                            else:
                                                self.logger.info(f"   ‚úì Level 2: {department_label} ({final_score:.3f})")

                                            predictions.append({
                                                "from_name": from_name_department,
                                                "to_name": to_name_department,
                                                "type": "taxonomy",
                                                "value": {
                                                    "taxonomy": [department_label.split(" > ")],
                                                    "score": float(final_score)
                                                }
                                            })
                                            
                                            self.logger.info(f"   ‚úì Level 2 prediction: {department_label}")

                                            # ========== LEVEL 3: QUERY TYPE ==========
                                            if self.querytype_model is not None:
                                                try:
                                                    conditional_text_qt = (
                                                        f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                                        f"Department: {department_label} "
                                                        f"Email: {current_text}"
                                                    )

                                                    qt_ids = tokenizer.encode(
                                                        conditional_text_qt,
                                                        add_special_tokens=True,
                                                        max_length=256,
                                                        truncation=True
                                                    )
                                                    qt_mask = [int(t > 0) for t in qt_ids]

                                                    qt_logits = self.querytype_model(
                                                        torch.tensor([qt_ids]).to(device),
                                                        torch.tensor([qt_mask]).to(device)
                                                    )

                                                    if isinstance(qt_logits, (tuple, list)):
                                                        qt_logits = qt_logits[0]

                                                    qt_probs = torch.softmax(qt_logits, dim=1)
                                                    
                                                    # ========== HIERARCHICAL CONSTRAINT FOR LEVEL 3 ==========
                                                    qt_encoder = self.querytype_model.get_encoder()
                                                    all_querytypes = qt_encoder.classes_
                                                    
                                                    # Filter to only QueryTypes that start with current department
                                                    valid_indices = []
                                                    valid_labels = []
                                                    
                                                    for idx, qt_label in enumerate(all_querytypes):
                                                        if qt_label.startswith(department_label):
                                                            valid_indices.append(idx)
                                                            valid_labels.append(qt_label)
                                                    
                                                    if len(valid_indices) == 0:
                                                        self.logger.warning(f"‚ö†Ô∏è  No QueryTypes found for department: {department_label}")
                                                        continue
                                                    
                                                    # Get probabilities only for valid indices
                                                    valid_probs = qt_probs[0][valid_indices]
                                                    
                                                    # Find the highest probability among valid options
                                                    best_valid_idx = torch.argmax(valid_probs).item()
                                                    qt_idx = valid_indices[best_valid_idx]
                                                    querytype_label = valid_labels[best_valid_idx]
                                                    final_score = valid_probs[best_valid_idx].item()
                                                    
                                                    # Log the constraint application
                                                    original_prediction_idx = torch.argmax(qt_probs, dim=1).item()
                                                    original_prediction = all_querytypes[original_prediction_idx]
                                                    
                                                    if original_prediction_idx != qt_idx:
                                                        self.logger.warning(f"‚ö†Ô∏è  Hierarchical constraint applied at Level 3!")
                                                        self.logger.warning(f"   Department: {department_label}")
                                                        self.logger.warning(f"   Original: {original_prediction} ({qt_probs[0][original_prediction_idx].item():.3f})")
                                                        self.logger.warning(f"   Corrected: {querytype_label} ({final_score:.3f})")
                                                    else:
                                                        self.logger.info(f"   ‚úì Level 3: {querytype_label} ({final_score:.3f})")

                                                    predictions.append({
                                                        "from_name": from_name_querytype,
                                                        "to_name": to_name_querytype,
                                                        "type": "taxonomy",
                                                        "value": {
                                                            "taxonomy": [querytype_label.split(" > ")],
                                                            "score": float(final_score)
                                                        }
                                                    })
                                                    
                                                except Exception as e:
                                                    self.logger.error(f"‚ùå Error in Level 3 prediction for item {i}: {e}")
                                                    self.logger.exception("Full traceback:")
                                                    
                                        except Exception as e:
                                            self.logger.error(f"‚ùå Error in Level 2 prediction for item {i}: {e}")
                                            self.logger.exception("Full traceback:")
```

---

## **Now Your Predictions Will Be:**

### **Before (Wrong):**
```
Level 1: Internet Banking
Level 2: Mobile Banking > App ‚ùå
Level 3: Mobile Banking > App > Unable to login ‚ùå
```

### **After (Correct):**
```
Level 1: Internet Banking
Level 2: Internet Banking > Account Access ‚úÖ (Corrected from "Mobile Banking > App")
Level 3: Internet Banking > Account Access > Unable to login ‚úÖ (Now correct parent)
```

---

## **What You'll See in Logs:**
```
‚úì Level 1 predictions: ['Internet Banking']
‚ö†Ô∏è  Hierarchical constraint applied at Level 2!
   MasterDepartment: Internet Banking
   Original prediction: Mobile Banking > App (score: 0.652)
   Corrected to: Internet Banking > Account Access (score: 0.487)
‚úì Level 2: Internet Banking > Account Access (score: 0.487)
‚ö†Ô∏è  Hierarchical constraint applied at Level 3!
   Department: Internet Banking > Account Access
   Original: Mobile Banking > App > Unable to login (score: 0.701)
   Corrected: Internet Banking > Account Access > Unable to login (score: 0.534)
‚úì Level 3: Internet Banking > Account Access > Unable to login (score: 0.534)






# Level 1 training were getting completed sucessfuly but not level 2 due so some lablesl which has 1 data each so we needed
to change train -test split logic 



# ========== TRAIN LEVEL 1: MasterDepartment ONLY ==========
self.logger.info("")
self.logger.info("=" * 80)
self.logger.info("üéØ LEVEL 1 TRAINING: MasterDepartment ONLY")
self.logger.info("=" * 80)

# Smart split: Keep single-sample classes in training only
class_counts = df['masterdepartment'].value_counts()
single_sample_classes = class_counts[class_counts == 1].index.tolist()
multi_sample_classes = class_counts[class_counts >= 2].index.tolist()

self.logger.info(f"üìä Classes with 1 sample: {len(single_sample_classes)} (will be in training only)")
self.logger.info(f"üìä Classes with 2+ samples: {len(multi_sample_classes)} (will be split)")

# Separate single-sample data (goes to training only)
single_sample_df = df[df['masterdepartment'].isin(single_sample_classes)]

# Separate multi-sample data (will be split)
multi_sample_df = df[df['masterdepartment'].isin(multi_sample_classes)]

if len(multi_sample_df) > 0:
    # Split only the multi-sample data
    min_samples = multi_sample_df['masterdepartment'].value_counts().min()
    
    if min_samples >= 2:
        self.logger.info(f"‚úì Using stratified split for multi-sample classes")
        try:
            level1_train_multi, level1_test_df = train_test_split(
                multi_sample_df, test_size=0.2, random_state=42,
                stratify=multi_sample_df['masterdepartment']
            )
        except ValueError as e:
            self.logger.warning(f"‚ö†Ô∏è  Stratified split failed: {e}")
            self.logger.warning("‚ö†Ô∏è  Falling back to random split")
            level1_train_multi, level1_test_df = train_test_split(
                multi_sample_df, test_size=0.2, random_state=42
            )
    else:
        # Shouldn't happen, but handle it
        level1_train_multi, level1_test_df = train_test_split(
            multi_sample_df, test_size=0.2, random_state=42
        )
    
    # Combine: all single-sample + training portion of multi-sample
    level1_train_df = pd.concat([single_sample_df, level1_train_multi], ignore_index=True)
else:
    # All classes have only 1 sample
    level1_train_df = single_sample_df
    level1_test_df = pd.DataFrame(columns=df.columns)  # Empty test set
    self.logger.warning("‚ö†Ô∏è  All classes have only 1 sample - no test set will be created")

self.logger.info(f"üìä Level 1 - Train: {len(level1_train_df)} | Test: {len(level1_test_df)}")
self._train_level1(level1_train_df, level1_test_df)








# ========== TRAIN LEVEL 2: Department ==========
self.logger.info("")
self.logger.info("=" * 80)
self.logger.info("üéØ LEVEL 2 TRAINING: Department")
self.logger.info("=" * 80)

# Filter samples with department labels FIRST
dept_df = df[df['department'].notna()].copy()

if len(dept_df) > 0:
    self.logger.info(f"üìä Department samples: {len(dept_df)}")
    
    # Smart split: Keep single-sample classes in training only
    dept_class_counts = dept_df['department'].value_counts()
    dept_single_sample_classes = dept_class_counts[dept_class_counts == 1].index.tolist()
    dept_multi_sample_classes = dept_class_counts[dept_class_counts >= 2].index.tolist()
    
    self.logger.info(f"üìä Departments with 1 sample: {len(dept_single_sample_classes)} (will be in training only)")
    self.logger.info(f"üìä Departments with 2+ samples: {len(dept_multi_sample_classes)} (will be split)")
    
    # Separate single-sample data
    dept_single_sample_df = dept_df[dept_df['department'].isin(dept_single_sample_classes)]
    
    # Separate multi-sample data
    dept_multi_sample_df = dept_df[dept_df['department'].isin(dept_multi_sample_classes)]
    
    if len(dept_multi_sample_df) > 0:
        # Split only the multi-sample data
        dept_min_samples = dept_multi_sample_df['department'].value_counts().min()
        
        if dept_min_samples >= 2:
            self.logger.info(f"‚úì Using stratified split for departments")
            try:
                dept_train_multi, dept_test_df = train_test_split(
                    dept_multi_sample_df, test_size=0.2, random_state=42,
                    stratify=dept_multi_sample_df['department']
                )
            except ValueError as e:
                self.logger.warning(f"‚ö†Ô∏è  Stratified split failed: {e}")
                self.logger.warning("‚ö†Ô∏è  Falling back to random split")
                dept_train_multi, dept_test_df = train_test_split(
                    dept_multi_sample_df, test_size=0.2, random_state=42
                )
        else:
            dept_train_multi, dept_test_df = train_test_split(
                dept_multi_sample_df, test_size=0.2, random_state=42
            )
        
        # Combine: all single-sample + training portion of multi-sample
        dept_train_df = pd.concat([dept_single_sample_df, dept_train_multi], ignore_index=True)
    else:
        # All departments have only 1 sample
        dept_train_df = dept_single_sample_df
        dept_test_df = pd.DataFrame(columns=dept_df.columns)
        self.logger.warning("‚ö†Ô∏è  All departments have only 1 sample - no test set will be created")
    
    self.logger.info(f"üìä Level 2 - Train: {len(dept_train_df)} | Test: {len(dept_test_df)}")
    self._train_level2(dept_train_df, dept_test_df)
else:
    self.logger.warning("‚ö†Ô∏è  No Department labels found - skipping Level 2 training")
















# ========== TRAIN LEVEL 3: QueryType ==========
self.logger.info("")
self.logger.info("=" * 80)
self.logger.info("üéØ LEVEL 3 TRAINING: QueryType")
self.logger.info("=" * 80)

# Filter samples with querytype labels FIRST
qt_df = df[df['querytype'].notna()].copy()

if len(qt_df) > 0:
    self.logger.info(f"üìä QueryType samples: {len(qt_df)}")
    
    # Smart split: Keep single-sample classes in training only
    qt_class_counts = qt_df['querytype'].value_counts()
    qt_single_sample_classes = qt_class_counts[qt_class_counts == 1].index.tolist()
    qt_multi_sample_classes = qt_class_counts[qt_class_counts >= 2].index.tolist()
    
    self.logger.info(f"üìä QueryTypes with 1 sample: {len(qt_single_sample_classes)} (will be in training only)")
    self.logger.info(f"üìä QueryTypes with 2+ samples: {len(qt_multi_sample_classes)} (will be split)")
    
    # Separate single-sample data
    qt_single_sample_df = qt_df[qt_df['querytype'].isin(qt_single_sample_classes)]
    
    # Separate multi-sample data
    qt_multi_sample_df = qt_df[qt_df['querytype'].isin(qt_multi_sample_classes)]
    
    if len(qt_multi_sample_df) > 0:
        # Split only the multi-sample data
        qt_min_samples = qt_multi_sample_df['querytype'].value_counts().min()
        
        if qt_min_samples >= 2:
            self.logger.info(f"‚úì Using stratified split for querytypes")
            try:
                qt_train_multi, qt_test_df = train_test_split(
                    qt_multi_sample_df, test_size=0.2, random_state=42,
                    stratify=qt_multi_sample_df['querytype']
                )
            except ValueError as e:
                self.logger.warning(f"‚ö†Ô∏è  Stratified split failed: {e}")
                self.logger.warning("‚ö†Ô∏è  Falling back to random split")
                qt_train_multi, qt_test_df = train_test_split(
                    qt_multi_sample_df, test_size=0.2, random_state=42
                )
        else:
            qt_train_multi, qt_test_df = train_test_split(
                qt_multi_sample_df, test_size=0.2, random_state=42
            )
        
        # Combine: all single-sample + training portion of multi-sample
        qt_train_df = pd.concat([qt_single_sample_df, qt_train_multi], ignore_index=True)
    else:
        # All querytypes have only 1 sample
        qt_train_df = qt_single_sample_df
        qt_test_df = pd.DataFrame(columns=qt_df.columns)
        self.logger.warning("‚ö†Ô∏è  All querytypes have only 1 sample - no test set will be created")
    
    self.logger.info(f"üìä Level 3 - Train: {len(qt_train_df)} | Test: {len(qt_test_df)}")
    self._train_level3(qt_train_df, qt_test_df)
else:
    self.logger.warning("‚ö†Ô∏è  No QueryType labels found - skipping Level 3 training")





























def _train_level1(self, train_df, test_df):
    """Train Level 1: MasterDepartment ONLY"""
    
    from sklearn.preprocessing import LabelEncoder
    
    # Check if we have test data
    has_test_data = len(test_df) > 0
    if not has_test_data:
        self.logger.warning("‚ö†Ô∏è  No test data available - will train without evaluation")
    
    # Encode labels
    masterdept_encoder = LabelEncoder()



# ========== EVALUATION ==========
training_logger.end_training()
        
        # ========== EVALUATION ==========
        if has_test_data:
            self.logger.info("")
            self.logger.info("üìä Evaluating Level 1 model...")
            
            try:
                model.eval()



self.logger.info("‚úÖ Level 1 evaluation complete")
                
            except Exception as e:
                self.logger.error(f"‚ùå Error during Level 1 evaluation: {e}")
                self.logger.exception("Full traceback:")
                raise
        else:
            # No test data - skip evaluation but still save model
            self.logger.warning("‚ö†Ô∏è  Skipping evaluation - no test data available")
            
            # Save model without evaluation
            model.set_encoder(masterdept_encoder)
            
            save_path = os.path.join(self.model_dir, self.finetuned_model_name)
            model.save(save_path)
            
            # Save tokenizer
            tokenizer.save_pretrained(save_path)
            
            self.logger.info(f"üíæ Level 1 model saved to {save_path} (without evaluation)")






def _train_level2(self, train_df, test_df):
    """Train Level 2: Department (conditioned on MasterDepartment)"""
    
    from sklearn.preprocessing import LabelEncoder
    
    # Check if we have test data
    has_test_data = len(test_df) > 0
    if not has_test_data:
        self.logger.warning("‚ö†Ô∏è  No test data available - will train Level 2 without evaluation")
    
    # Encode labels
    dept_encoder = LabelEncoder()




training_logger.end_training()
        
        # ========== EVALUATION ==========
        if has_test_data:
            self.logger.info("")
            self.logger.info("üìä Evaluating Level 2 model...")
            
            try:
                model.eval()






self.logger.info("‚úÖ Level 2 evaluation complete")
                
            except Exception as e:
                self.logger.error(f"‚ùå Error during Level 2 evaluation: {e}")
                self.logger.exception("Full traceback:")
                raise
        else:
            # No test data - skip evaluation but still save model
            self.logger.warning("‚ö†Ô∏è  Skipping Level 2 evaluation - no test data available")
            
            # Save model without evaluation
            model.set_encoder(dept_encoder)
            
            save_path = os.path.join(self.model_dir, "department_model")
            model.save(save_path)
            
            self.logger.info(f"üíæ Level 2 model saved to {save_path} (without evaluation)")

















def _train_level3(self, train_df, test_df):
    """Train Level 3: QueryType (conditioned on MasterDepartment + Department)"""
    
    from sklearn.preprocessing import LabelEncoder
    
    # Check if we have test data
    has_test_data = len(test_df) > 0
    if not has_test_data:
        self.logger.warning("‚ö†Ô∏è  No test data available - will train Level 3 without evaluation")
    
    # Encode labels
    qt_encoder = LabelEncoder()



training_logger.end_training()
        
        # ========== EVALUATION ==========
        if has_test_data:
            self.logger.info("")
            self.logger.info("üìä Evaluating Level 3 model...")
            
            try:
                model.eval()




self.logger.info("‚úÖ Level 3 evaluation complete")
                
            except Exception as e:
                self.logger.error(f"‚ùå Error during Level 3 evaluation: {e}")
                self.logger.exception("Full traceback:")
                raise
        else:
            # No test data - skip evaluation but still save model
            self.logger.warning("‚ö†Ô∏è  Skipping Level 3 evaluation - no test data available")
            
            # Save model without evaluation
            model.set_encoder(qt_encoder)
            
            save_path = os.path.join(self.model_dir, "querytype_model")
            model.save(save_path)



            
            self.logger.info(f"üíæ Level 3 model saved to {save_path} (without evaluation)")










#to check if level 1 already exists then start traing level 2 
# ========== TRAIN LEVEL 1: MasterDepartment ONLY ==========
self.logger.info("")
self.logger.info("=" * 80)
self.logger.info("üéØ LEVEL 1 TRAINING: MasterDepartment ONLY")
self.logger.info("=" * 80)

# Check if Level 1 model already exists
level1_model_path = os.path.join(self.model_dir, self.finetuned_model_name)
level1_classifier_exists = os.path.exists(os.path.join(level1_model_path, 'masterdepartment_classifier.pth'))
level1_weights_exists = os.path.exists(os.path.join(level1_model_path, 'model.safetensors'))
level1_encoder_exists = os.path.exists(os.path.join(level1_model_path, 'masterdepartment_encoder.pkl'))

level1_model_exists = level1_classifier_exists and level1_weights_exists and level1_encoder_exists

if level1_model_exists:
    self.logger.info("=" * 80)
    self.logger.info("‚úÖ LEVEL 1 MODEL ALREADY EXISTS - SKIPPING TRAINING")
    self.logger.info("=" * 80)
    self.logger.info(f"üìÅ Model path: {level1_model_path}")
    self.logger.info(f"   ‚úì masterdepartment_classifier.pth")
    self.logger.info(f"   ‚úì model.safetensors")
    self.logger.info(f"   ‚úì masterdepartment_encoder.pkl")
    self.logger.info("üí° To retrain Level 1: Delete the masterdepartment_model folder")
    self.logger.info("=" * 80)
else:
    # Smart split: Keep single-sample classes in training only
    class_counts = df['masterdepartment'].value_counts()