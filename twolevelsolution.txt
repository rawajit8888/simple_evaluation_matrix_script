def fit(self, event, data, tasks, **kwargs):
        """
        3-Level Hierarchical Training Pipeline
        
        Level 1: Email â†’ MasterDepartment
        Level 2: Email + MasterDepartment â†’ Department
        Level 3: Email + MasterDepartment + Department â†’ QueryType
        """
        
        self.logger.info("=" * 80)
        self.logger.info("ðŸš€ 3-LEVEL HIERARCHICAL TRAINING INITIATED")
        self.logger.info("=" * 80)
        
        # Initialize database
        self.init_metrics_db()
        
        # ========== PREPARE DATA ==========
        self.logger.info("ðŸ“Š Preparing training data...")
        
        train_data = []
        for task in tasks:
            extracted = self._extract_taxonomy_from_task(task)
            if extracted and extracted['text']:
                train_data.append(extracted)
        
        if len(train_data) == 0:
            self.logger.error("âŒ No valid training data found")
            self.logger.error("Please check your Label Studio annotations:")
            self.logger.error("  - Ensure tasks have 'masterdepartment' labels")
            self.logger.error("  - Check that taxonomy fields are properly configured")
            return
        
        self.logger.info(f"âœ“ Prepared {len(train_data)} training samples")
        
        # Log sample for debugging
        if train_data:
            sample = train_data[0]
            self.logger.info(f"ðŸ“ Sample annotation:")
            self.logger.info(f"   MasterDepartment: {sample['masterdepartment']}")
            self.logger.info(f"   Department: {sample['department']}")
            self.logger.info(f"   QueryType: {sample['querytype']}")
        
        # Log distribution
        masterdept_count = sum(1 for d in train_data if d['masterdepartment'])
        dept_count = sum(1 for d in train_data if d['department'])
        qt_count = sum(1 for d in train_data if d['querytype'])
        
        self.logger.info(f"ðŸ“Š Label distribution:")
        self.logger.info(f"   Level 1 (MasterDepartment): {masterdept_count} samples")
        self.logger.info(f"   Level 2 (Department): {dept_count} samples")
        self.logger.info(f"   Level 3 (QueryType): {qt_count} samples")
        
        # Convert to DataFrame
        df = pd.DataFrame(train_data)
        
        from sklearn.model_selection import train_test_split
        
        # ========== TRAIN LEVEL 1: MasterDepartment ONLY ==========
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("ðŸŽ¯ LEVEL 1 TRAINING: MasterDepartment ONLY")
        self.logger.info("=" * 80)
        
        # Split for Level 1
        class_counts = df['masterdepartment'].value_counts()
        min_samples = class_counts.min()
        
        if min_samples >= 2:
            self.logger.info(f"âœ“ Using stratified split (min class size: {min_samples})")
            try:
                level1_train_df, level1_test_df = train_test_split(
                    df, test_size=0.2, random_state=42, 
                    stratify=df['masterdepartment']
                )
            except ValueError as e:
                self.logger.warning(f"âš ï¸  Stratified split failed: {e}")
                self.logger.warning("âš ï¸  Falling back to random split")
                level1_train_df, level1_test_df = train_test_split(
                    df, test_size=0.2, random_state=42
                )
        else:
            self.logger.warning(f"âš ï¸  Some classes have only 1 sample - using random split")
            level1_train_df, level1_test_df = train_test_split(
                df, test_size=0.2, random_state=42
            )
        
        self.logger.info(f"ðŸ“Š Level 1 - Train: {len(level1_train_df)} | Test: {len(level1_test_df)}")
        self._train_level1(level1_train_df, level1_test_df)
        
        # ========== TRAIN LEVEL 2: Department ==========
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("ðŸŽ¯ LEVEL 2 TRAINING: Department")
        self.logger.info("=" * 80)
        
        # Filter samples with department labels FIRST
        dept_df = df[df['department'].notna()].copy()
        
        if len(dept_df) > 0:
            self.logger.info(f"ðŸ“Š Department samples: {len(dept_df)}")
            
            # NOW split the filtered data
            dept_class_counts = dept_df['department'].value_counts()
            dept_min_samples = dept_class_counts.min()
            
            if dept_min_samples >= 2:
                self.logger.info(f"âœ“ Using stratified split for departments (min class size: {dept_min_samples})")
                try:
                    dept_train_df, dept_test_df = train_test_split(
                        dept_df, test_size=0.2, random_state=42,
                        stratify=dept_df['department']
                    )
                except ValueError as e:
                    self.logger.warning(f"âš ï¸  Stratified split failed: {e}")
                    self.logger.warning("âš ï¸  Falling back to random split")
                    dept_train_df, dept_test_df = train_test_split(
                        dept_df, test_size=0.2, random_state=42
                    )
            else:
                self.logger.warning(f"âš ï¸  Some department classes have only 1 sample - using random split")
                dept_train_df, dept_test_df = train_test_split(
                    dept_df, test_size=0.2, random_state=42
                )
            
            self.logger.info(f"ðŸ“Š Level 2 - Train: {len(dept_train_df)} | Test: {len(dept_test_df)}")
            self._train_level2(dept_train_df, dept_test_df)
        else:
            self.logger.warning("âš ï¸  No Department labels found - skipping Level 2 training")
        
        # ========== TRAIN LEVEL 3: QueryType ==========
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("ðŸŽ¯ LEVEL 3 TRAINING: QueryType")
        self.logger.info("=" * 80)
        
        # Filter samples with querytype labels FIRST
        qt_df = df[df['querytype'].notna()].copy()
        
        if len(qt_df) > 0:
            self.logger.info(f"ðŸ“Š QueryType samples: {len(qt_df)}")
            
            # NOW split the filtered data
            qt_class_counts = qt_df['querytype'].value_counts()
            qt_min_samples = qt_class_counts.min()
            
            if qt_min_samples >= 2:
                self.logger.info(f"âœ“ Using stratified split for querytypes (min class size: {qt_min_samples})")
                try:
                    qt_train_df, qt_test_df = train_test_split(
                        qt_df, test_size=0.2, random_state=42,
                        stratify=qt_df['querytype']
                    )
                except ValueError as e:
                    self.logger.warning(f"âš ï¸  Stratified split failed: {e}")
                    self.logger.warning("âš ï¸  Falling back to random split")
                    qt_train_df, qt_test_df = train_test_split(
                        qt_df, test_size=0.2, random_state=42
                    )
            else:
                self.logger.warning(f"âš ï¸  Some querytype classes have only 1 sample - using random split")
                qt_train_df, qt_test_df = train_test_split(
                    qt_df, test_size=0.2, random_state=42
                )
            
            self.logger.info(f"ðŸ“Š Level 3 - Train: {len(qt_train_df)} | Test: {len(qt_test_df)}")
            self._train_level3(qt_train_df, qt_test_df)
        else:
            self.logger.warning("âš ï¸  No QueryType labels found - skipping Level 3 training")
        
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("ðŸŽ‰ 3-LEVEL HIERARCHICAL TRAINING COMPLETE")
        self.logger.info("=" * 80)







#evaluatin matrix save to folder 

def save_evaluation_to_files(self, level, level_name, test_df, true_labels, pred_labels, accuracy, f1):
        """
        Save evaluation results to files:
        - CSV with test predictions
        - Text file with classification report
        - Text file with confusion matrix
        """
        from sklearn.metrics import classification_report, confusion_matrix
        
        # Create evaluation folder
        eval_dir = os.path.join(self.model_dir, "evaluation", level_name)
        os.makedirs(eval_dir, exist_ok=True)
        
        self.logger.info(f"ðŸ“ Creating evaluation folder: {eval_dir}")
        
        # ========== SAVE TEST PREDICTIONS TO CSV ==========
        predictions_df = test_df.copy()
        predictions_df['true_label'] = true_labels
        predictions_df['predicted_label'] = pred_labels
        predictions_df['correct'] = predictions_df['true_label'] == predictions_df['predicted_label']
        
        csv_path = os.path.join(eval_dir, f"{level_name}_test_predictions.csv")
        predictions_df.to_csv(csv_path, index=False)
        self.logger.info(f"ðŸ’¾ Test predictions saved to: {csv_path}")
        
        # ========== SAVE CLASSIFICATION REPORT ==========
        report = classification_report(true_labels, pred_labels, zero_division=0)
        report_path = os.path.join(eval_dir, f"{level_name}_classification_report.txt")
        
        with open(report_path, 'w') as f:
            f.write(f"=" * 80 + "\n")
            f.write(f"CLASSIFICATION REPORT - {level_name.upper()}\n")
            f.write(f"=" * 80 + "\n")
            f.write(f"Overall Accuracy: {accuracy:.4f}\n")
            f.write(f"Overall F1-Score: {f1:.4f}\n")
            f.write(f"=" * 80 + "\n\n")
            f.write(report)
        
        self.logger.info(f"ðŸ’¾ Classification report saved to: {report_path}")
        
        # ========== SAVE CONFUSION MATRIX ==========
        cm = confusion_matrix(true_labels, pred_labels)
        cm_path = os.path.join(eval_dir, f"{level_name}_confusion_matrix.txt")
        
        with open(cm_path, 'w') as f:
            f.write(f"=" * 80 + "\n")
            f.write(f"CONFUSION MATRIX - {level_name.upper()}\n")
            f.write(f"=" * 80 + "\n\n")
            f.write(str(cm))
            f.write("\n")
        
        self.logger.info(f"ðŸ’¾ Confusion matrix saved to: {cm_path}")
        
        # ========== SAVE SUMMARY ==========
        summary_path = os.path.join(eval_dir, f"{level_name}_summary.txt")
        
        with open(summary_path, 'w') as f:
            f.write(f"=" * 80 + "\n")
            f.write(f"EVALUATION SUMMARY - {level_name.upper()}\n")
            f.write(f"=" * 80 + "\n")
            f.write(f"Level: {level}\n")
            f.write(f"Training samples: {len(test_df)}\n")
            f.write(f"Accuracy: {accuracy:.4f}\n")
            f.write(f"F1-Score: {f1:.4f}\n")
            f.write(f"Number of classes: {len(set(true_labels))}\n")
            f.write(f"Correct predictions: {sum(predictions_df['correct'])}\n")
            f.write(f"Incorrect predictions: {sum(~predictions_df['correct'])}\n")
            f.write(f"=" * 80 + "\n")
        
        self.logger.info(f"ðŸ’¾ Summary saved to: {summary_path}")
        self.logger.info(f"âœ… Evaluation files created in: {eval_dir}")





# Save evaluation to files
        self.save_evaluation_to_files(
            level=1,
            level_name="masterdepartment",
            test_df=test_df,
            true_labels=decoded_masterdept_true,
            pred_labels=decoded_masterdept_pred,
            accuracy=masterdept_accuracy,
            f1=masterdept_f1
        )










# Save evaluation to files
        self.save_evaluation_to_files(
            level=2,
            level_name="department",
            test_df=test_df,
            true_labels=decoded_dept_true,
            pred_labels=decoded_dept_pred,
            accuracy=dept_accuracy,
            f1=dept_f1
        )











# Save evaluation to files
        self.save_evaluation_to_files(
            level=3,
            level_name="querytype",
            test_df=test_df,
            true_labels=decoded_qt_true,
            pred_labels=decoded_qt_pred,
            accuracy=qt_accuracy,
            f1=qt_f1
        )
```

---

## **Summary of Changes:**

**1. Line ~768**: Add the new `save_evaluation_to_files()` method (65 lines)

**2. Line 1352**: Add evaluation file saving call for Level 1

**3. Line 1489**: Add evaluation file saving call for Level 2

**4. Line 1626**: Add evaluation file saving call for Level 3

---

## **What This Creates:**

After training, you'll get this folder structure:
```
./results/bert-classification-sentiment3level_training/
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ masterdepartment/
â”‚   â”‚   â”œâ”€â”€ masterdepartment_test_predictions.csv
â”‚   â”‚   â”œâ”€â”€ masterdepartment_classification_report.txt
â”‚   â”‚   â”œâ”€â”€ masterdepartment_confusion_matrix.txt
â”‚   â”‚   â””â”€â”€ masterdepartment_summary.txt
â”‚   â”œâ”€â”€ department/
â”‚   â”‚   â”œâ”€â”€ department_test_predictions.csv
â”‚   â”‚   â”œâ”€â”€ department_classification_report.txt
â”‚   â”‚   â”œâ”€â”€ department_confusion_matrix.txt
â”‚   â”‚   â””â”€â”€ department_summary.txt
â”‚   â””â”€â”€ querytype/
â”‚       â”œâ”€â”€ querytype_test_predictions.csv
â”‚       â”œâ”€â”€ querytype_classification_report.txt
â”‚       â”œâ”€â”€ querytype_confusion_matrix.txt
â”‚       â””â”€â”€ querytype_summary.txt
â”œâ”€â”€ metrics.db  (SQLite database - still created)
â”œâ”€â”€ masterdepartment_model/
â”œâ”€â”€ department_model/
â””â”€â”€ querytype_model/














#prediction server error 500
def predict(self, tasks: List[Dict], texts: str, context: Optional[Dict] = None, **kwargs):
        """
        3-LEVEL HIERARCHICAL INFERENCE PIPELINE
        
        Just pass email text, automatically routes through:
        1. Level 1: Email â†’ MasterDepartment
        2. Level 2: Email + MasterDepartment â†’ Department
        3. Level 3: Email + MasterDepartment + Department â†’ QueryType
        
        Returns all predictions in a single result.
        """
        
        try:
            self.logger.info("=" * 80)
            self.logger.info("ðŸ” PREDICTION REQUEST RECEIVED")
            self.logger.info("=" * 80)
            
            # ========== VALIDATION ==========
            if self.model is None:
                error_msg = "âŒ Level 1 model not loaded - cannot make predictions"
                self.logger.error(error_msg)
                return []
            
            if self.tokenizer is None:
                error_msg = "âŒ Tokenizer not loaded - cannot make predictions"
                self.logger.error(error_msg)
                return []
            
            # NOTE: label_interface is optional - only needed for Label Studio mode
            
            self.logger.info("âœ“ All required components loaded")

            # ========== GET LABEL STUDIO TAGS (Optional for external calls) ==========
            try:
                if self.label_interface is not None:
                    # Label Studio mode - get proper tags
                    def getMasterDepartmentAttrName(attrs):
                        return attrs == "masterdepartment"

                    def getDepartmentAttrName(attrs):
                        return attrs == "department"

                    def getQueryTypeAttrName(attrs):
                        return attrs == "querytype"

                    from_name_masterdepartment, to_name_masterdepartment, _ = \
                        self.label_interface.get_first_tag_occurence(
                            'Taxonomy', 'HyperText', getMasterDepartmentAttrName
                        )

                    from_name_department, to_name_department, _ = \
                        self.label_interface.get_first_tag_occurence(
                            "Taxonomy", "HyperText", getDepartmentAttrName
                        )

                    from_name_querytype, to_name_querytype, _ = \
                        self.label_interface.get_first_tag_occurence(
                            "Taxonomy", "HyperText", getQueryTypeAttrName
                        )
                    
                    self.logger.info("âœ“ Label Studio tags retrieved successfully")
                else:
                    # External API mode - use default tags
                    from_name_masterdepartment = "masterdepartment"
                    to_name_masterdepartment = "text"
                    from_name_department = "department"
                    to_name_department = "text"
                    from_name_querytype = "querytype"
                    to_name_querytype = "text"
                    
                    self.logger.info("âœ“ Using default tags (external API mode)")
                
            except Exception as e:
                self.logger.error(f"âŒ Error getting tags: {e}")
                # Use defaults as fallback
                from_name_masterdepartment = "masterdepartment"
                to_name_masterdepartment = "text"
                from_name_department = "department"
                to_name_department = "text"
                from_name_querytype = "querytype"
                to_name_querytype = "text"
                self.logger.info("âœ“ Using fallback default tags")

            # ========== EXTRACT TEXT FROM INPUT ==========
            try:
                self.logger.info(f"ðŸ“ Input type: {type(texts)}")
                self.logger.info(f"ðŸ“ Input length: {len(texts) if isinstance(texts, (list, str)) else 'N/A'}")
                
                text_list = []
                
                if isinstance(texts, str):
                    # Single string
                    text_list = [texts]
                    self.logger.info("âœ“ Processing single string input")
                    
                elif isinstance(texts, list):
                    # List of texts or dicts
                    for idx, text in enumerate(texts):
                        if isinstance(text, dict):
                            if 'text' in text:
                                text_list.append(text['text'])
                            elif 'html' in text:
                                text_list.append(text['html'])
                            else:
                                text_list.append(str(text))
                        elif isinstance(text, str):
                            text_list.append(text)
                        else:
                            text_list.append(str(text))
                    
                    self.logger.info(f"âœ“ Processing list with {len(text_list)} items")
                    
                elif isinstance(texts, dict):
                    # Single dict
                    if 'text' in texts:
                        text_list = [texts['text']]
                    elif 'html' in texts:
                        text_list = [texts['html']]
                    else:
                        text_list = [str(texts)]
                    
                    self.logger.info("âœ“ Processing single dict input")
                    
                else:
                    # Fallback
                    text_list = [str(texts)]
                    self.logger.warning(f"âš ï¸  Unknown input type, converting to string")
                
                if not text_list or all(not t for t in text_list):
                    error_msg = "âŒ No valid text found in input"
                    self.logger.error(error_msg)
                    return []
                
                self.logger.info(f"ðŸ“Š Successfully extracted {len(text_list)} text(s) for processing")
                
                # Log first text sample for debugging
                if text_list:
                    sample_text = text_list[0][:100] + "..." if len(text_list[0]) > 100 else text_list[0]
                    self.logger.info(f"ðŸ“„ Sample text: {sample_text}")
                
            except Exception as e:
                self.logger.error(f"âŒ Error extracting text from input: {e}")
                self.logger.exception("Full traceback:")
                return []

            # ========== TOKENIZATION ==========
            try:
                tokenizer = self.tokenizer
                MAX_LEN = 256
                batch_size = 16

                self.logger.info(f"ðŸ”¤ Tokenizing {len(text_list)} text(s)...")
                
                # Tokenize and pad
                _inputs, _masks = self._tokenize_and_pad(text_list, tokenizer, MAX_LEN)
                
                self.logger.info(f"âœ“ Tokenization complete - Shape: {_inputs.shape}")

                # Create DataLoader
                dataloader = self._create_dataloader(
                    _inputs, _masks, batch_size=batch_size, shuffle=False
                )
                
                self.logger.info(f"âœ“ DataLoader created - {len(dataloader)} batches")
                
            except Exception as e:
                self.logger.error(f"âŒ Error during tokenization: {e}")
                self.logger.exception("Full traceback:")
                return []

            # ========== SETUP MODELS ==========
            try:
                device = self._get_device()
                self.logger.info(f"ðŸ–¥ï¸  Using device: {device}")
                
                # Move models to device and set to eval mode
                self.model.to(device)
                self.model.eval()
                self.logger.info("âœ“ Level 1 model ready")

                if self.department_model is not None:
                    self.department_model.to(device)
                    self.department_model.eval()
                    self.logger.info("âœ“ Level 2 model ready - Department predictions enabled")
                else:
                    self.logger.info("âš ï¸  Level 2 model not loaded - Department predictions will be skipped")

                if self.querytype_model is not None:
                    self.querytype_model.to(device)
                    self.querytype_model.eval()
                    self.logger.info("âœ“ Level 3 model ready - QueryType predictions enabled")
                else:
                    self.logger.info("âš ï¸  Level 3 model not loaded - QueryType predictions will be skipped")
                    
            except Exception as e:
                self.logger.error(f"âŒ Error setting up models: {e}")
                self.logger.exception("Full traceback:")
                return []

            # ========== RUN INFERENCE ==========
            try:
                self.logger.info("ðŸš€ Starting inference...")
                
                predictions = []
                global_text_idx = 0

                with torch.no_grad():
                    for batch_idx, batch in enumerate(dataloader):
                        self.logger.info(f"   Processing batch {batch_idx + 1}/{len(dataloader)}...")
                        
                        input_ids, attention_mask = [t.to(device) for t in batch]
            
                        # ========== LEVEL 1: MasterDepartment ==========
                        try:
                            masterdept_logits = self.model(input_ids, attention_mask)
                            masterdept_probs = torch.softmax(masterdept_logits, dim=1)
                            masterdept_preds = torch.argmax(masterdept_probs, dim=1)

                            decoded_masterdept_preds = self._decode_predictions(
                                masterdept_preds, 'masterdepartment'
                            )
                            
                            self.logger.info(f"   âœ“ Level 1 predictions: {decoded_masterdept_preds}")
                            
                        except Exception as e:
                            self.logger.error(f"âŒ Error in Level 1 prediction: {e}")
                            self.logger.exception("Full traceback:")
                            raise

                        for i in range(len(masterdept_preds)):
                            try:
                                current_text = text_list[global_text_idx]
                                global_text_idx += 1

                                # ---------- MASTER DEPARTMENT ----------
                                predictions.append({
                                    "from_name": from_name_masterdepartment,
                                    "to_name": to_name_masterdepartment,
                                    "type": "taxonomy",
                                    "value": {
                                        "taxonomy": [
                                            [decoded_masterdept_preds[i]]
                                        ],
                                        "score": float(masterdept_probs[i][masterdept_preds[i]].item())
                                    }
                                })

                                # ========== LEVEL 2: DEPARTMENT ==========
                                if self.department_model is not None:
                                    try:
                                        conditional_text = (
                                            f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                            f"Email: {current_text}"
                                        )

                                        dept_ids = tokenizer.encode(
                                            conditional_text,
                                            add_special_tokens=True,
                                            max_length=256,
                                            truncation=True
                                        )
                                        dept_mask = [int(t > 0) for t in dept_ids]

                                        dept_logits = self.department_model(
                                            torch.tensor([dept_ids]).to(device),
                                            torch.tensor([dept_mask]).to(device)
                                        )

                                        if isinstance(dept_logits, (tuple, list)):
                                            dept_logits = dept_logits[0]

                                        dept_probs = torch.softmax(dept_logits, dim=1)
                                        dept_idx = torch.argmax(dept_probs, dim=1).item()

                                        department_label = self._decode_predictions(
                                            [dept_idx], "department"
                                        )[0]

                                        predictions.append({
                                            "from_name": from_name_department,
                                            "to_name": to_name_department,
                                            "type": "taxonomy",
                                            "value": {
                                                "taxonomy": [department_label.split(" > ")],
                                                "score": float(dept_probs[0][dept_idx].item())
                                            }
                                        })
                                        
                                        self.logger.info(f"   âœ“ Level 2 prediction: {department_label}")

                                        # ========== LEVEL 3: QUERY TYPE ==========
                                        if self.querytype_model is not None:
                                            try:
                                                conditional_text_qt = (
                                                    f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                                    f"Department: {department_label} "
                                                    f"Email: {current_text}"
                                                )

                                                qt_ids = tokenizer.encode(
                                                    conditional_text_qt,
                                                    add_special_tokens=True,
                                                    max_length=256,
                                                    truncation=True
                                                )
                                                qt_mask = [int(t > 0) for t in qt_ids]

                                                qt_logits = self.querytype_model(
                                                    torch.tensor([qt_ids]).to(device),
                                                    torch.tensor([qt_mask]).to(device)
                                                )

                                                if isinstance(qt_logits, (tuple, list)):
                                                    qt_logits = qt_logits[0]

                                                qt_probs = torch.softmax(qt_logits, dim=1)
                                                qt_idx = torch.argmax(qt_probs, dim=1).item()

                                                querytype_label = self._decode_predictions(
                                                    [qt_idx], "querytype"
                                                )[0]

                                                predictions.append({
                                                    "from_name": from_name_querytype,
                                                    "to_name": to_name_querytype,
                                                    "type": "taxonomy",
                                                    "value": {
                                                        "taxonomy": [querytype_label.split(" > ")],
                                                        "score": float(qt_probs[0][qt_idx].item())
                                                    }
                                                })
                                                
                                                self.logger.info(f"   âœ“ Level 3 prediction: {querytype_label}")
                                                
                                            except Exception as e:
                                                self.logger.error(f"âŒ Error in Level 3 prediction for item {i}: {e}")
                                                self.logger.exception("Full traceback:")
                                                # Continue without Level 3 prediction
                                                
                                    except Exception as e:
                                        self.logger.error(f"âŒ Error in Level 2 prediction for item {i}: {e}")
                                        self.logger.exception("Full traceback:")
                                        # Continue without Level 2 and Level 3 predictions
                                        
                            except Exception as e:
                                self.logger.error(f"âŒ Error processing item {i} in batch: {e}")
                                self.logger.exception("Full traceback:")
                                # Continue to next item

                self.logger.info("=" * 80)
                self.logger.info(f"âœ… INFERENCE COMPLETE - Generated {len(predictions)} predictions")
                self.logger.info("=" * 80)
                
                return predictions
                
            except Exception as e:
                self.logger.error(f"âŒ Error during inference loop: {e}")
                self.logger.exception("Full traceback:")
                return []
            
        except Exception as e:
            # Catch-all for any unexpected errors
            self.logger.error("=" * 80)
            self.logger.error("âŒ CRITICAL ERROR IN PREDICT METHOD")
            self.logger.error("=" * 80)
            self.logger.error(f"Error: {e}")
            self.logger.exception("Full traceback:")
            
            # Return empty list instead of crashing
            return []


# change logic for level 3 training

# ========== LEVEL 3: QUERY TYPE ==========
                                        if self.querytype_model is not None:
                                            try:
                                                conditional_text_qt = (
                                                    f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                                    f"Department: {department_label} "
                                                    f"Email: {current_text}"
                                                )

                                                qt_ids = tokenizer.encode(
                                                    conditional_text_qt,
                                                    add_special_tokens=True,
                                                    max_length=256,
                                                    truncation=True
                                                )
                                                qt_mask = [int(t > 0) for t in qt_ids]

                                                qt_logits = self.querytype_model(
                                                    torch.tensor([qt_ids]).to(device),
                                                    torch.tensor([qt_mask]).to(device)
                                                )

                                                if isinstance(qt_logits, (tuple, list)):
                                                    qt_logits = qt_logits[0]

                                                qt_probs = torch.softmax(qt_logits, dim=1)
                                                
                                                # ========== HIERARCHICAL CONSTRAINT ==========
                                                qt_encoder = self.querytype_model.get_encoder()
                                                all_querytypes = qt_encoder.classes_
                                                
                                                # Filter to only QueryTypes that start with current department
                                                valid_indices = []
                                                valid_labels = []
                                                
                                                for idx, qt_label in enumerate(all_querytypes):
                                                    if qt_label.startswith(department_label):
                                                        valid_indices.append(idx)
                                                        valid_labels.append(qt_label)
                                                
                                                if len(valid_indices) == 0:
                                                    self.logger.warning(f"âš ï¸  No QueryTypes found for department: {department_label}")
                                                    continue
                                                
                                                # Get probabilities only for valid indices
                                                valid_probs = qt_probs[0][valid_indices]
                                                
                                                # Find the highest probability among valid options
                                                best_valid_idx = torch.argmax(valid_probs).item()
                                                qt_idx = valid_indices[best_valid_idx]
                                                querytype_label = valid_labels[best_valid_idx]
                                                final_score = valid_probs[best_valid_idx].item()
                                                
                                                # Log the constraint application
                                                original_prediction_idx = torch.argmax(qt_probs, dim=1).item()
                                                original_prediction = all_querytypes[original_prediction_idx]
                                                
                                                if original_prediction_idx != qt_idx:
                                                    self.logger.warning(f"âš ï¸  Hierarchical constraint applied!")
                                                    self.logger.warning(f"   Original: {original_prediction} ({qt_probs[0][original_prediction_idx].item():.3f})")
                                                    self.logger.warning(f"   Corrected: {querytype_label} ({final_score:.3f})")
                                                else:
                                                    self.logger.info(f"   âœ“ Level 3: {querytype_label} ({final_score:.3f})")

                                                predictions.append({
                                                    "from_name": from_name_querytype,
                                                    "to_name": to_name_querytype,
                                                    "type": "taxonomy",
                                                    "value": {
                                                        "taxonomy": [querytype_label.split(" > ")],
                                                        "score": float(final_score)
                                                    }
                                                })
                                                
                                            except Exception as e:
                                                self.logger.error(f"âŒ Error in Level 3 prediction for item {i}: {e}")
                                                self.logger.exception("Full traceback:")





#changed logic for level 2 predictions
# ========== LEVEL 2: DEPARTMENT ==========
                                    if self.department_model is not None:
                                        try:
                                            conditional_text = (
                                                f"MasterDepartment: {decoded_masterdept_preds[i]} "
                                                f"Email: {current_text}"
                                            )

                                            dept_ids = tokenizer.encode(
                                                conditional_text,
                                                add_special_tokens=True,
                                                max_length=256,
                                                truncation=True
                                            )
                                            dept_mask = [int(t > 0) for t in dept_ids]

                                            dept_logits = self.department_model(
                                                torch.tensor([dept_ids]).to(device),
                                                torch.tensor([dept_mask]).to(device)
                                            )

                                            if isinstance(dept_logits, (tuple, list)):
                                                dept_logits = dept_logits[0]

                                            dept_probs = torch.softmax(dept_logits, dim=1)
                                            
                                            # ========== HIERARCHICAL CONSTRAINT FOR LEVEL 2 ==========
                                            dept_encoder = self.department_model.get_encoder()
                                            all_departments = dept_encoder.classes_
                                            
                                            current_masterdept = decoded_masterdept_preds[i]
                                            
                                            # Filter to only Departments that start with current MasterDepartment
                                            valid_indices = []
                                            valid_labels = []
                                            
                                            for idx, dept_label in enumerate(all_departments):
                                                if dept_label.startswith(current_masterdept):
                                                    valid_indices.append(idx)
                                                    valid_labels.append(dept_label)
                                            
                                            if len(valid_indices) == 0:
                                                self.logger.warning(f"âš ï¸  No Departments found for MasterDepartment: {current_masterdept}")
                                                continue
                                            
                                            # Get probabilities only for valid indices
                                            valid_probs = dept_probs[0][valid_indices]
                                            
                                            # Find the highest probability among valid options
                                            best_valid_idx = torch.argmax(valid_probs).item()
                                            dept_idx = valid_indices[best_valid_idx]
                                            department_label = valid_labels[best_valid_idx]
                                            final_score = valid_probs[best_valid_idx].item()
                                            
                                            # Log the constraint application
                                            original_prediction_idx = torch.argmax(dept_probs, dim=1).item()
                                            original_prediction = all_departments[original_prediction_idx]
                                            
                                            if original_prediction_idx != dept_idx:
                                                self.logger.warning(f"âš ï¸  Hierarchical constraint applied at Level 2!")
                                                self.logger.warning(f"   MasterDepartment: {current_masterdept}")
                                                self.logger.warning(f"   Original: {original_prediction} ({dept_probs[0][original_prediction_idx].item():.3f})")
                                                self.logger.warning(f"   Corrected: {department_label} ({final_score:.3f})")
                                            else:
                                                self.logger.info(f"   âœ“ Level 2: {department_label} ({final_score:.3f})")

                                            predictions.append({
                                                "from_name": from_name_department,
                                                "to_name": to_name_department,
                                                "type": "taxonomy",
                                                "value": {
                                                    "taxonomy": [department_label.split(" > ")],
                                                    "score": float(final_score)
                                                }
                                            })
                                            
                                            self.logger.info(f"   âœ“ Level 2 prediction: {department_label}")

